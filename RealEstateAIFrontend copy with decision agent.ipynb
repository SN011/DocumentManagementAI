{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install webrtcvad\n",
    "# !pip install pygame\n",
    "# !pip install pyaudio webrtcvad \n",
    "# !pip install google-cloud-texttospeech\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.initialize_groq import init_groq\n",
    "from tools.file_mgmt_tools import FileOrganizerTool, MoveFileTool, CreateFolderTool, FolderMovementTool, ImprovedSearchTool\n",
    "from tools.document_tools import GoogleDocWriteTool\n",
    "from tools.miscellaneous_mgmt import GmailSendPdfTool, GoogleSheetsUpdateTool\n",
    "\n",
    "client,llm = init_groq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import texttospeech\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate, \n",
    "    SystemMessagePromptTemplate, \n",
    "    HumanMessagePromptTemplate, \n",
    "    MessagesPlaceholder, \n",
    "    PromptTemplate\n",
    ")\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain.agents import create_structured_chat_agent, AgentExecutor\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_community.tools import HumanInputRun\n",
    "import tools.initialize_groq\n",
    "import langchain_core\n",
    "import typing\n",
    "\n",
    "prompt = ChatPromptTemplate(\n",
    "    input_variables=['agent_scratchpad', 'input', 'tool_names', 'tools'],\n",
    "    input_types={\n",
    "        'chat_history': typing.List[\n",
    "            typing.Union[\n",
    "                langchain_core.messages.ai.AIMessage, \n",
    "                langchain_core.messages.human.HumanMessage, \n",
    "                langchain_core.messages.chat.ChatMessage, \n",
    "                langchain_core.messages.system.SystemMessage, \n",
    "                langchain_core.messages.function.FunctionMessage, \n",
    "                langchain_core.messages.tool.ToolMessage\n",
    "            ]\n",
    "        ]\n",
    "    },\n",
    "    metadata={\n",
    "        'lc_hub_owner': 'hwchase17',\n",
    "        'lc_hub_repo': 'structured-chat-agent',\n",
    "        'lc_hub_commit_hash': 'ea510f70a5872eb0f41a4e3b7bb004d5711dc127adee08329c664c6c8be5f13c'\n",
    "    },\n",
    "    messages=[\n",
    "        SystemMessagePromptTemplate(\n",
    "            prompt=PromptTemplate(\n",
    "                input_variables=['tool_names', 'tools'],\n",
    "                template=(\n",
    "                    'You are a document management assistant proficient in using GSuite tools. '\n",
    "                    'Your role is to assist the user in managing their documents efficiently. '\n",
    "                    'You have access to the following tools:\\n\\n{tools}\\n\\n'\n",
    "                    'Use a JSON blob to specify a tool by providing an action key (tool name) and an action_input key (tool input).\\n\\n'\n",
    "                    'Valid \"action\" values: \"Final Answer\" or {tool_names}\\n\\n'\n",
    "                    'Provide only ONE action per $JSON_BLOB, as shown:\\n\\n'\n",
    "                    '```\\n{{\\n  \"action\": $TOOL_NAME,\\n  \"action_input\": $INPUT\\n}}\\n```\\n\\n'\n",
    "                    'Follow this format:\\n\\n'\n",
    "                    'Question: input question to answer\\n'\n",
    "                    'Thought: consider previous and subsequent steps\\n'\n",
    "                    'Action:\\n```\\n$JSON_BLOB\\n```\\n'\n",
    "                    'Observation: action result\\n... (repeat Thought/Action/Observation N times)\\n'\n",
    "                    'Thought: I know what to respond\\n'\n",
    "                    'Action:\\n```\\n{{\\n  \"action\": \"Final Answer\",\\n  \"action_input\": \"Final response to human\"\\n}}\\n\\n'\n",
    "                    'Begin! Remember to ALWAYS respond with a valid JSON blob of a single action. '\n",
    "                    'Use tools if necessary and respond directly if appropriate. '\n",
    "                    'Ensure you gather all necessary information by interacting with the user. '\n",
    "                    'Format is Action:```$JSON_BLOB```then Observation.'\n",
    "                )\n",
    "            )\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name='chat_history', optional=True),\n",
    "        HumanMessagePromptTemplate(\n",
    "            prompt=PromptTemplate(\n",
    "                input_variables=['agent_scratchpad', 'input'],\n",
    "                template='{input}\\n\\n{agent_scratchpad}\\n(reminder to respond in a JSON blob no matter what)'\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "human_prompt = ChatPromptTemplate(\n",
    "    input_variables=['agent_scratchpad', 'input', 'tool_names', 'tools'],\n",
    "    input_types={\n",
    "        'chat_history': typing.List[\n",
    "            typing.Union[\n",
    "                langchain_core.messages.ai.AIMessage, \n",
    "                langchain_core.messages.human.HumanMessage, \n",
    "                langchain_core.messages.chat.ChatMessage, \n",
    "                langchain_core.messages.system.SystemMessage, \n",
    "                langchain_core.messages.function.FunctionMessage, \n",
    "                langchain_core.messages.tool.ToolMessage\n",
    "            ]\n",
    "        ]\n",
    "    },\n",
    "    metadata={\n",
    "        'lc_hub_owner': 'hwchase17',\n",
    "        'lc_hub_repo': 'structured-chat-agent',\n",
    "        'lc_hub_commit_hash': 'ea510f70a5872eb0f41a4e3b7bb004d5711dc127adee08329c664c6c8be5f13c'\n",
    "    },\n",
    "    messages=[\n",
    "        SystemMessagePromptTemplate(\n",
    "            prompt=PromptTemplate(\n",
    "                input_variables=['tool_names', 'tools'],\n",
    "                template=(\n",
    "                    'Your role is to fulfill the desire of user in the most accurate and detailed way possible. '\n",
    "                    \n",
    "                    'You have access to the following tools:\\n\\n{tools}\\n\\n'\n",
    "                    'Use a JSON blob to specify a tool by providing an action key (tool name) and an action_input key (tool input).\\n\\n'\n",
    "                    'Valid \"action\" values: \"Final Answer\" or {tool_names}\\n\\n'\n",
    "                    'Provide only ONE action per $JSON_BLOB, as shown:\\n\\n'\n",
    "                    '```\\n{{\\n  \"action\": $TOOL_NAME,\\n  \"action_input\": $INPUT\\n}}\\n```\\n\\n'\n",
    "                    'Follow this format:\\n\\n'\n",
    "                    'Question: input question to answer\\n'\n",
    "                    'Thought: consider previous and subsequent steps\\n'\n",
    "                    'Action:\\n```\\n$JSON_BLOB\\n```\\n'\n",
    "                    'Observation: action result\\n... (repeat Thought/Action/Observation N times)\\n'\n",
    "                    'Thought: I know what to respond\\n'\n",
    "                    'Action:\\n```\\n{{\\n  \"action\": \"Final Answer\",\\n  \"action_input\": \"Final response to human\"\\n}}\\n\\n'\n",
    "                    'Begin! Remember to ALWAYS respond with a valid JSON blob of a single action. '\n",
    "                    'Use tools if necessary and respond directly if appropriate. '\n",
    "                    'Ensure you gather all necessary information by interacting with the user. '\n",
    "                    'Format is Action:```$JSON_BLOB```then Observation.'\n",
    "                )\n",
    "            )\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name='chat_history', optional=True),\n",
    "        HumanMessagePromptTemplate(\n",
    "            prompt=PromptTemplate(\n",
    "                input_variables=['agent_scratchpad', 'input'],\n",
    "                template='{input}\\n\\n{agent_scratchpad}\\n(reminder to respond in a JSON blob no matter what)'\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'except' statement on line 140 (859346357.py, line 143)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[4], line 143\u001b[1;36m\u001b[0m\n\u001b[1;33m    def transcribe_audio():\u001b[0m\n\u001b[1;37m                           ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block after 'except' statement on line 140\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify, send_file, render_template\n",
    "import whisper\n",
    "import pyaudio\n",
    "import wave\n",
    "import webrtcvad\n",
    "import collections\n",
    "from google.cloud import texttospeech\n",
    "import random\n",
    "import asyncio\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import aiofiles\n",
    "from flask_cors import CORS\n",
    "import requests\n",
    "import logging\n",
    "import os\n",
    "from tools.imports import *\n",
    "import tools.initialize_groq\n",
    "from dotenv import load_dotenv\n",
    "from langchain import hub\n",
    "from flask_socketio import SocketIO, emit\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize tools and credentials\n",
    "credentials_path = os.getenv('CREDENTIALS_PATH')\n",
    "tts_service_acct_path = os.getenv('SERVICE_ACCOUNT_PATH')\n",
    "audio_path = os.getenv('AUDIO_PATH')\n",
    "tts_synthesis_path = os.getenv('TTS_SYNTHESIS')\n",
    "\n",
    "# Initialize TTS client\n",
    "tts_client = texttospeech.TextToSpeechClient.from_service_account_file(tts_service_acct_path)\n",
    "\n",
    "# Initialize tools\n",
    "my_tools = [\n",
    "    GoogleDocWriteTool(credentials_path),\n",
    "    GoogleSheetsUpdateTool(credentials_path),\n",
    "    GmailSendPdfTool(credentials_path),\n",
    "    MoveFileTool(credentials_path),\n",
    "    CreateFolderTool(credentials_path),\n",
    "    FolderMovementTool(credentials_path),\n",
    "    FileOrganizerTool(credentials_path),\n",
    "    ImprovedSearchTool(credentials_path),\n",
    "    \n",
    "]\n",
    "\n",
    "# Initialize LLM API key\n",
    "llm.groq_api_key = random.choice(tools.initialize_groq.api_keys)\n",
    "\n",
    "# Initialize logger\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format='%(asctime)s [%(threadName)s] %(levelname)s: %(message)s',\n",
    "    handlers=[logging.StreamHandler()]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize Flask app\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "socketio = SocketIO(app, cors_allowed_origins=\"*\")\n",
    "\n",
    "# Global variables\n",
    "chat_history = []\n",
    "model = whisper.load_model(\"base\")\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "RATE = 16000\n",
    "CHUNK = 1024\n",
    "is_recording = False\n",
    "\n",
    "# Initialize PyAudio and VAD\n",
    "audio = pyaudio.PyAudio()\n",
    "vad = webrtcvad.Vad(3)\n",
    "\n",
    "# Executor for handling asynchronous tasks\n",
    "executor = ThreadPoolExecutor(max_workers=5)\n",
    "\n",
    "# Credentials dictionary\n",
    "credentials = {\"name\": \"\", \"email\": \"\", \"recemail\": \"\", \"phone\": \"\"}\n",
    "\n",
    "@app.route('/start_recording', methods=['POST'])\n",
    "def start_recording():\n",
    "    global is_recording\n",
    "    is_recording = True\n",
    "    record_audio()\n",
    "    return jsonify({\"status\": \"recording started\"})\n",
    "\n",
    "@app.route('/stop_recording', methods=['POST'])\n",
    "def stop_recording():\n",
    "    global is_recording\n",
    "    is_recording = False\n",
    "    return jsonify({\"status\": \"recording stopped\"})\n",
    "\n",
    "def record_audio(**kwargs):\n",
    "    global is_recording\n",
    "    logger.debug('Starting audio recording...')\n",
    "    try:\n",
    "        stream = audio.open(format=FORMAT, channels=CHANNELS, rate=RATE, input=True, frames_per_buffer=CHUNK)\n",
    "        frames = []\n",
    "        ring_buffer = collections.deque(maxlen=100)\n",
    "        triggered = False\n",
    "        voiced_frames = []\n",
    "        silence_threshold = 10\n",
    "        silence_chunks = 0\n",
    "\n",
    "        while is_recording:\n",
    "            data = stream.read(CHUNK)\n",
    "            frames.append(data)\n",
    "            num_subframes = int(len(data) / 320)\n",
    "            for i in range(num_subframes):\n",
    "                subframe = data[i*320:(i+1)*320]\n",
    "                is_speech = vad.is_speech(subframe, RATE)\n",
    "                ring_buffer.append((subframe, is_speech))\n",
    "            num_voiced = len([f for f, speech in ring_buffer if speech])\n",
    "\n",
    "            if not triggered:\n",
    "                if num_voiced > 0.6 * ring_buffer.maxlen:\n",
    "                    triggered = True\n",
    "                    voiced_frames.extend([f for f, s in ring_buffer])\n",
    "                    ring_buffer.clear()\n",
    "            else:\n",
    "                voiced_frames.append(data)\n",
    "                if num_voiced < 0.2 * ring_buffer.maxlen:\n",
    "                    silence_chunks += 1\n",
    "                    if silence_chunks > silence_threshold:\n",
    "                        triggered = False\n",
    "                        break\n",
    "                else:\n",
    "                    silence_chunks = 0\n",
    "\n",
    "        stream.stop_stream()\n",
    "        stream.close()\n",
    "\n",
    "        with wave.open(audio_path, 'wb') as wf:\n",
    "            wf.setnchannels(CHANNELS)\n",
    "            wf.setsampwidth(audio.get_sample_size(FORMAT))\n",
    "            wf.setframerate(RATE)\n",
    "            wf.writeframes(b''.join(voiced_frames))\n",
    "        logger.debug('Audio recording completed and file saved.')\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred while recording audio: {e}\")\n",
    "\n",
    "def transcribe_audio():\n",
    "    result = model.transcribe(audio_path)\n",
    "    transcription = result['text']\n",
    "    logger.debug(f'Audio transcription completed: {transcription}')\n",
    "    return transcription\n",
    "\n",
    "# async def ai_response(transcription: str):\n",
    "#     global chat_history\n",
    "#     logger.debug(f'Generating AI response for transcription: {transcription}')\n",
    "    \n",
    "#     user_input = transcription\n",
    "#     human_tool = HumanInputRun(prompt_func=text_to_speech_prompt, input_func=speech_to_text_input)\n",
    "#     human_agent = create_structured_chat_agent(llm=llm, tools=[human_tool], prompt=human_prompt)\n",
    "#     agent_executor = AgentExecutor(\n",
    "#         agent=human_agent,\n",
    "#         tools=[human_tool],\n",
    "#         verbose=True,\n",
    "#         handle_parsing_errors=True,\n",
    "#         return_intermediate_steps=True,\n",
    "#     )\n",
    "#     result = agent_executor.invoke({\"input\": user_input})\n",
    "#     chat_history.append({\"input\": user_input, \"response\": result})\n",
    "    \n",
    "#     asyncio.create_task(handle_response_with_agents(result['output']))\n",
    "#     await synthesize_speech(result['output'], tts_synthesis_path)\n",
    "    \n",
    "#     return result['output']\n",
    "\n",
    "\n",
    "\n",
    "async def ai_response(transcription: str):\n",
    "    global chat_history\n",
    "    logger.debug(f'Generating AI response for transcription: {transcription}')\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"\"\"You are a nice, great assistant. User will tell you things. You just respond. \n",
    "                YOU SHALL NOT INDICATE ANY TOOL USE UNTIL YOU KNOW YOU HAVE EVERYTHING YOU NEED.\n",
    "                DO NOT ASSUME USER WANTS TO WRITE TO A DOCUMENT OR DO ANYTHING ELSE UNLESS YOU ARE 100% SURE!!!!!UNDERSTAND??????!!!!!! OR ELSE I WILL BECOME ANGRY\n",
    "                If what the user says is one of these  you must explicitly say AT THE END OF YOUR RESPONSE in this very format depending on which tool - \"I will use the {[(tool.name + \" tool, \") for tool in my_tools]}\"\n",
    "                so that user can confirm if you got it correctly. \n",
    "                \n",
    "                IMPORTANT: IF YOU REMEMBER YOU HAVE USED A TOOL ALREADY (REFER TO CHAT HISTORY), AND YOU ARE IN THE PROCESS OF USING IT AGAIN, \\\n",
    "                YOU MUST ASK USER IF THEY ARE SURE TO USE THE TOOL AGAIN OR NOT.\n",
    "\n",
    "                \n",
    "                If user tells you to do something that is not one of these, you kindly say that you don't have access to that functionality.\n",
    "                \"\"\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": transcription + \"Here is the chat history for context (NEVER TALK ABOUT CHAT HISTORY. IT IS ONLY FOR YOU! NEVER TALK ABOUT IT IN YOUR RESPONSES!!!!): [\" + str(chat_history) + \"]\"\n",
    "            }\n",
    "        ],\n",
    "        model=\"llama3-70b-8192\",\n",
    "    )\n",
    "    response = chat_completion.choices[0].message.content\n",
    "    logger.debug(f'AI response generated: {response}')\n",
    "    chat_history.append(\"USER: \" + transcription + \"\\nTHE AI MODEL: \" + response + \"\\n\")\n",
    "    \n",
    "    if 'I will use' in response:\n",
    "        asyncio.create_task(handle_response_with_agents(response))\n",
    "   \n",
    "    await synthesize_speech(response)\n",
    "\n",
    "    return response\n",
    "my_tools.append(HumanInputRun(prompt_func=ai_response, input_func=record_audio))\n",
    "async def handle_response_with_agents(response):\n",
    "    logger.debug(f'Handling response with agents: {response}')\n",
    "    await asyncio.get_event_loop().run_in_executor(executor, handle_agents, response)\n",
    "\n",
    "def handle_agents(response):\n",
    "    logger.debug(f'Processing response with agents: {response}')\n",
    "\n",
    "    cc_out = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"please turn this 'response' into a request for a doc manager having the following capabilities {[tool.name for tool in my_tools]}. AN EXAMPLE: \\nHey can you please [do ONLY WORD FOR WORD whatever is in the response]? \\n AND HERE IS RESPONSE: \\n\" + response\n",
    "            }\n",
    "        ],\n",
    "        model='llama3-70b-8192',\n",
    "    ).choices[0].message.content\n",
    "\n",
    "    response = cc_out\n",
    "    name = credentials['name']\n",
    "    email = credentials['email']\n",
    "    recemail = credentials[\"recemail\"]\n",
    "    phone = credentials['phone']\n",
    "    \n",
    "    response += \"\\n\\n Here is extra info you will need: \\nCredentials:\\n\" + str(credentials) + \"\\nTHE CHAT HISTORY: \\n\" + str(chat_history)\n",
    "\n",
    "    # Set the Groq API key randomly\n",
    "    llm.groq_api_key = random.choice(tools.initialize_groq.api_keys)\n",
    "\n",
    "    search_agent = create_structured_chat_agent(llm, my_tools, prompt)\n",
    "    agent_executor = AgentExecutor(\n",
    "        agent=search_agent,\n",
    "        tools=my_tools,\n",
    "        verbose=True,\n",
    "        handle_parsing_errors=True,\n",
    "        return_intermediate_steps=True,\n",
    "    )\n",
    "    \n",
    "    result = agent_executor.invoke({\"input\": response})\n",
    "    chat_history.append({\"input\": response, \"response\": result})\n",
    "    mystr = (str(result['intermediate_steps']) + \"\\n\" + str(result['output']))\n",
    "\n",
    "    for step in result['intermediate_steps']:\n",
    "        print('STEP:',step)\n",
    "        output_file = f\"intermediate_output_{step['index']}.mp3\"\n",
    "        synth_speech(step['observation'], output_file)\n",
    "\n",
    "    final_response = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"please sanitize this input so that someone can speak it. START THE SPEAKABLE INPUT WITH '@' symbol. AFTER THE @ SYMBOL ONLY THAT OUTPUT SHOULD BE THERE! NOTHING ELSE! Here is input\\n \" + mystr\n",
    "            }\n",
    "        ],\n",
    "        model='llama3-70b-8192',\n",
    "    ).choices[0].message.content\n",
    "\n",
    "    synth_speech(final_response, tts_synthesis_path)\n",
    "    print('SYNTH SPEECH DONE AT THE END OF HANDLING AGENTS')\n",
    "    return result\n",
    "\n",
    "def synth_speech(text, output_file=None):\n",
    "    logger.debug(f'Starting speech synthesis for text: {text}')\n",
    "    synthesis_input = texttospeech.SynthesisInput(text=text)\n",
    "    voice = texttospeech.VoiceSelectionParams(\n",
    "        language_code=\"en-US\",\n",
    "        name=\"en-US-Casual-K\"\n",
    "    )\n",
    "    audio_config = texttospeech.AudioConfig(\n",
    "        audio_encoding=texttospeech.AudioEncoding.MP3\n",
    "    )\n",
    "    response = tts_client.synthesize_speech(\n",
    "        input=synthesis_input, voice=voice, audio_config=audio_config\n",
    "    )\n",
    "    \n",
    "    if output_file:\n",
    "        with open(output_file, 'wb') as out:\n",
    "            out.write(response.audio_content)\n",
    "    logger.debug('Speech synthesis completed and file saved.')\n",
    "\n",
    "async def synthesize_speech(text, output_file=tts_synthesis_path):\n",
    "    logger.debug(f'Starting speech synthesis for text: {text}')\n",
    "    synthesis_input = texttospeech.SynthesisInput(text=text)\n",
    "    voice = texttospeech.VoiceSelectionParams(\n",
    "        language_code=\"en-US\",\n",
    "        name=\"en-US-Casual-K\"\n",
    "    )\n",
    "    audio_config = texttospeech.AudioConfig(\n",
    "        audio_encoding=texttospeech.AudioEncoding.MP3\n",
    "    )\n",
    "    response = await asyncio.get_event_loop().run_in_executor(\n",
    "        executor, lambda: tts_client.synthesize_speech(\n",
    "            input=synthesis_input, voice=voice, audio_config=audio_config\n",
    "        )\n",
    "    )\n",
    "    if output_file:\n",
    "        async with aiofiles.open(output_file, 'wb') as out:\n",
    "            await out.write(response.audio_content)\n",
    "    logger.debug('Speech synthesis completed and file saved.')\n",
    "\n",
    "@app.route('/set_credentials', methods=['POST'])\n",
    "def set_credentials():\n",
    "    global credentials\n",
    "    data = request.get_json()\n",
    "    if not data:\n",
    "        return jsonify({\"status\": \"failed\", \"message\": \"No data received\"}), 400\n",
    "    credentials['name'] = data.get('name')\n",
    "    credentials['email'] = data.get('email')\n",
    "    credentials['recemail'] = data.get('recemail')\n",
    "    credentials['phone'] = data.get('phone')\n",
    "    logger.info(\"THE CREDENTIALS ****** -------------> \", credentials)\n",
    "    return jsonify({\"status\": \"success\"})\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template('index2.html')\n",
    "\n",
    "@app.route('/voice_assistant')\n",
    "def voice_assistant():\n",
    "    return render_template('index2.html')\n",
    "\n",
    "@app.route('/authenticate', methods=['POST'])\n",
    "def authenticate():\n",
    "    auth_header = request.headers.get('Authorization')\n",
    "    token = auth_header.split(' ')[1] if auth_header else None\n",
    "\n",
    "    if not token:\n",
    "        return jsonify({'error': 'Missing token'}), 400\n",
    "\n",
    "    response = requests.get(\n",
    "        'https://www.googleapis.com/oauth2/v3/userinfo',\n",
    "        headers={'Authorization': f'Bearer {token}'}\n",
    "    )\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        return jsonify({'error': 'Failed to fetch user info'}), response.status_code\n",
    "\n",
    "    user_info = response.json()\n",
    "    return jsonify(user_info), 200\n",
    "\n",
    "@app.route('/talk', methods=['POST'])\n",
    "async def talk():\n",
    "    loop = asyncio.get_event_loop()\n",
    "    \n",
    "    global is_recording\n",
    "    if is_recording:\n",
    "        return jsonify({\"error\": \"Recording is still in progress\"}), 400\n",
    "    \n",
    "    logger.debug('Starting audio transcription...')\n",
    "    transcription = await loop.run_in_executor(executor, transcribe_audio)\n",
    "    logger.debug(f'Audio transcription completed: {transcription}')\n",
    "    \n",
    "    logger.debug('Generating AI response...')\n",
    "    ai_resp = await ai_response(transcription)\n",
    "    logger.debug(f'AI response generated: {ai_resp}')\n",
    "    \n",
    "    return jsonify({'response': ai_resp})\n",
    "\n",
    "@app.route('/get_audio')\n",
    "def get_audio():\n",
    "    return send_file(tts_synthesis_path, mimetype=\"audio/mp3\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
