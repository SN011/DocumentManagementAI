{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tools\n",
    "from tools.imports import *\n",
    "from tools.document_tools import GoogleDocWriteTool\n",
    "from tools.file_mgmt_tools import MoveFileTool, CreateFolderTool, FolderMovementTool, FileOrganizerTool, ImprovedSearchTool\n",
    "from tools.miscellaneous_mgmt import GoogleDriveUploadTool, GmailSendPdfTool, GoogleSheetsUpdateTool\n",
    "from tools.initialize_groq import init_groq\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client, llm = init_groq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials_path = os.getenv('CREDENTIALS_PATH')\n",
    "my_tools = []\n",
    "my_tools.extend(\n",
    "    \n",
    "    [GoogleDocWriteTool(credentials_path),\n",
    "    GoogleSheetsUpdateTool(credentials_path),\n",
    "    GmailSendPdfTool(credentials_path),\n",
    "    MoveFileTool(credentials_path),\n",
    "    CreateFolderTool(credentials_path),\n",
    "    FolderMovementTool(credentials_path),\n",
    "    FileOrganizerTool(credentials_path),\n",
    "    ImprovedSearchTool(credentials_path),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[GoogleDocWriteTool(credentials_path='paths/client_secret_291175256673-gr5p5vf3pi2h0m46h5qnd3ila4iitfqs.apps.googleusercontent.com.json', creds=<google.oauth2.credentials.Credentials object at 0x000001A455F51AC0>, docs_service=<googleapiclient.discovery.Resource object at 0x000001A455FFAF90>, drive_service=<googleapiclient.discovery.Resource object at 0x000001A456047380>),\n",
       " GoogleSheetsUpdateTool(credentials_path='paths/client_secret_291175256673-gr5p5vf3pi2h0m46h5qnd3ila4iitfqs.apps.googleusercontent.com.json', spreadsheet_id='1TSFyiTwctC1tABr2RQouBzLRMCG4RZ7lXdTVi-I58Mo', range_name='Sheet1', creds=<google.oauth2.credentials.Credentials object at 0x000001A4560477D0>, service=<googleapiclient.discovery.Resource object at 0x000001A4570F42F0>),\n",
       " GmailSendPdfTool(credentials_path='paths/client_secret_291175256673-gr5p5vf3pi2h0m46h5qnd3ila4iitfqs.apps.googleusercontent.com.json', creds=<google.oauth2.credentials.Credentials object at 0x000001A4570F4500>, service=<googleapiclient.discovery.Resource object at 0x000001A45714D3A0>),\n",
       " MoveFileTool(credentials_path='paths/client_secret_291175256673-gr5p5vf3pi2h0m46h5qnd3ila4iitfqs.apps.googleusercontent.com.json', creds=<google.oauth2.credentials.Credentials object at 0x000001A45714D580>, service=<googleapiclient.discovery.Resource object at 0x000001A4571B57C0>),\n",
       " CreateFolderTool(credentials_path='paths/client_secret_291175256673-gr5p5vf3pi2h0m46h5qnd3ila4iitfqs.apps.googleusercontent.com.json', creds=<google.oauth2.credentials.Credentials object at 0x000001A4571B5BE0>, service=<googleapiclient.discovery.Resource object at 0x000001A457219DC0>),\n",
       " FolderMovementTool(credentials_path='paths/client_secret_291175256673-gr5p5vf3pi2h0m46h5qnd3ila4iitfqs.apps.googleusercontent.com.json', creds=<google.oauth2.credentials.Credentials object at 0x000001A45721A1E0>, service=<googleapiclient.discovery.Resource object at 0x000001A457296420>),\n",
       " FileOrganizerTool(credentials_path='paths/client_secret_291175256673-gr5p5vf3pi2h0m46h5qnd3ila4iitfqs.apps.googleusercontent.com.json', creds=<google.oauth2.credentials.Credentials object at 0x000001A457296810>, service=<googleapiclient.discovery.Resource object at 0x000001A4572FEA50>),\n",
       " ImprovedSearchTool(credentials_path='paths/client_secret_291175256673-gr5p5vf3pi2h0m46h5qnd3ila4iitfqs.apps.googleusercontent.com.json', creds=<google.oauth2.credentials.Credentials object at 0x000001A4572FEE70>, output_dir='drive_batches', map_output_dir='mapped_batches', reduce_output_dir='final_aggregated', service=<googleapiclient.discovery.Resource object at 0x000001A45736F0B0>)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tools.initialize_groq\n",
    "# import random\n",
    "# from langchain.prompts import PromptTemplate\n",
    "# from langchain.agents import initialize_agent, AgentType, AgentExecutor\n",
    "# from google.cloud import texttospeech\n",
    "# from langchain.agents import load_agent\n",
    "\n",
    "\n",
    "# # Define the template\n",
    "# template = \"\"\"\n",
    "# YOU ARE A VERY ADVANCED DOCUMENT MANAGER WHO USES GOOGLE DRIVE FOR DOCUMENT MANAGEMENT.\n",
    "# WHEN USER INDICATES THEY WANT TO MOVE SOMETHING INTO GOOGLE DRIVE OR MY DRIVE, YOU PASS IN 'ROOT'!!!!! OKAY!!!!!\n",
    "# RESPOND IN A CLEAR CUT MANNER.\n",
    "# DO NOT SAY THINGS LIKE - 'here is the response' and the like. OKAY!!?!??\n",
    "# YOU SHALL NOT INDICATE ANY TOOL USE UNTIL YOU KNOW YOU HAVE EVERYTHING YOU NEED.\n",
    "# DO NOT ASSUME USER WANTS TO DO ANYTHING AT ALL UNLESS YOU ARE 100% SURE!!!!! UNDERSTAND??????!!!!!! OR ELSE I WILL BECOME ANGRY\n",
    "# BE REALLY CAREFUL WITH FILE AND FOLDER ID'S! ANY WRONG ID'S WILL RESULT IN FAILURE OF OPERATIONS\n",
    "\n",
    "# If user tells you to do something that is not one of these tools/operations,\\\n",
    "#       you kindly say that you don't have access to that functionality.\n",
    "\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# # User credentials\n",
    "# credentials = {\n",
    "#     \"name\": \"Gautham Ramachandran\",\n",
    "#     \"email\": \"sriramnallani35@gmail.com\",\n",
    "#     \"recemail\": \"gauthamramachandran3@gmail.com\",\n",
    "#     \"phone\": \"5715996302\"\n",
    "# }\n",
    "\n",
    "\n",
    "# def synthesize_speech(text, output_file):\n",
    "#     print('THE TEXT: ', text)\n",
    "#     idx = text.rfind('@')\n",
    "#     text = text[idx + 1:].strip()\n",
    "#     print('MODIFIED RESPONSE = ', text)\n",
    "#     tts_service_acct_path = os.getenv('SERVICE_ACCOUNT_PATH')\n",
    "#     client = texttospeech.TextToSpeechClient.from_service_account_file(tts_service_acct_path)\n",
    "\n",
    "    \n",
    "#     input_text = texttospeech.SynthesisInput(text=text)\n",
    "#     voice = texttospeech.VoiceSelectionParams(\n",
    "#         language_code=\"en-US\",\n",
    "#         ssml_gender=texttospeech.SsmlVoiceGender.MALE\n",
    "#     )\n",
    "#     audio_config = texttospeech.AudioConfig(\n",
    "#         audio_encoding=texttospeech.AudioEncoding.MP3\n",
    "#     )\n",
    "#     response = client.synthesize_speech(\n",
    "#         input=input_text,\n",
    "#         voice=voice,\n",
    "#         audio_config=audio_config\n",
    "#     )\n",
    "#     with open(output_file, \"wb\") as out:\n",
    "#         out.write(response.audio_content)\n",
    "#     print(f'Audio content written to \"{output_file}\"')\n",
    "\n",
    "\n",
    "# agent = None\n",
    "# # Define a function to run the agent and handle the response\n",
    "# def run_agent(input_text):\n",
    "#     global chat_history\n",
    "#     input_text += \"ALWAYS GENERATE THE GOOGLE DOC CONTENT YOURSELF! NEVER SPEAK TO HUMAN UNLESS INSTRUCTED. \\\n",
    "#             DO ONLY WHAT IS SAID IN INSTRUCTIONS, AND FORMAT YOUR RESPONSES ONLY AS SAID IN INSTRUCTIONS OR ELSE LIFE WILL END AS WE KNOW IT. \\\n",
    "#             DO EVERYTHING SAID HERE. NOT EVEN ONE THING SHALL BE LEFT INCOMPLETE.\\\n",
    "#             DO NOT CREATE FOLDERS WHEN ORGANIZING FILES.\\\n",
    "#             UNLESS USER TELLS YOU TO WRITE SOMETHING IN GOOGLE DOC DO NOT PASS ANYTHING INTO 'input_text' PARAMETER! \\\n",
    "#             ALSO, WHEN MOVING A FOLDER INTO ANOTHER FOLDER, ALWAYS USE FOLDER MOVEMENT TOOL\\n\" + template + \"\\n\"\n",
    "#     thepath = \"C:\\\\DEV\\\\Webdevfolder\\\\realestateai\\\\documents\\\\renovationcontract.pdf\"\n",
    "#     input_text += (\"\\n\\n Here is extra info you will need: \\nCredentials:\\n\" + str(credentials) + \"\\n\"\n",
    "#                    + \"Path to file on local system:\\n\" + thepath + \"\\nTHE CHAT HISTORY: \\n\"+str(chat_history))\n",
    "#     print(input_text)\n",
    "#     llm.groq_api_key = random.choice(tools.initialize_groq.api_keys)\n",
    "\n",
    "#     agent = initialize_agent(\n",
    "#         tools=my_tools,\n",
    "#         llm=llm,\n",
    "#         agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "#         handle_parsing_errors=True,\n",
    "#         verbose=True,\n",
    "#         max_iterations=1000,\n",
    "#         return_intermediate_steps=True\n",
    "#     )\n",
    "    \n",
    "#     result = agent.invoke({\n",
    "#         \"input\": input_text,\n",
    "#     })\n",
    "#     chat_history.append({\"input\": input_text, \"response\": result})\n",
    "#     mystr = (str(result['intermediate_steps']) + \"\\n\" + str(result['output']))\n",
    "    \n",
    "    \n",
    "#     synthesize_speech(client.chat.completions.create(\n",
    "#         messages=[\n",
    "#             {\n",
    "#                 \"role\": \"user\",\n",
    "#                 \"content\":\"please sanitize this input so that someone can speak it. START THE SPEAKABLE INPUT WITH '@' symbol: \" + mystr\n",
    "#             }\n",
    "#         ],\n",
    "#         model='llama3-70b-8192',\n",
    "#     ).choices[0].message.content, \"intermediateoutput.mp3\"\n",
    "#     )\n",
    "\n",
    "#     return result\n",
    "\n",
    "# user_input = f\"\"\" \n",
    "# move the folder called 'colab notebooks' into 'fcps google backup'\n",
    "# \"\"\" \n",
    "# result = run_agent(user_input)\n",
    "# print(result['output'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import asyncio\n",
    "import pyaudio\n",
    "import wave\n",
    "import threading\n",
    "import keyboard  # Add this import\n",
    "from google.cloud import texttospeech\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate, \n",
    "    SystemMessagePromptTemplate, \n",
    "    HumanMessagePromptTemplate, \n",
    "    MessagesPlaceholder, \n",
    "    PromptTemplate\n",
    ")\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain.agents import create_structured_chat_agent, AgentExecutor\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_community.tools import HumanInputRun\n",
    "import tools.initialize_groq\n",
    "import typing\n",
    "import langchain_core\n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "import whisper\n",
    "\n",
    "prompt = ChatPromptTemplate(\n",
    "    input_variables=['agent_scratchpad', 'input', 'tool_names', 'tools'],\n",
    "    input_types={\n",
    "        'chat_history': typing.List[\n",
    "            typing.Union[\n",
    "                langchain_core.messages.ai.AIMessage, \n",
    "                langchain_core.messages.human.HumanMessage, \n",
    "                langchain_core.messages.chat.ChatMessage, \n",
    "                langchain_core.messages.system.SystemMessage, \n",
    "                langchain_core.messages.function.FunctionMessage, \n",
    "                langchain_core.messages.tool.ToolMessage\n",
    "            ]\n",
    "        ]\n",
    "    },\n",
    "    metadata={\n",
    "        'lc_hub_owner': 'hwchase17',\n",
    "        'lc_hub_repo': 'structured-chat-agent',\n",
    "        'lc_hub_commit_hash': 'ea510f70a5872eb0f41a4e3b7bb004d5711dc127adee08329c664c6c8be5f13c'\n",
    "    },\n",
    "    messages=[\n",
    "        SystemMessagePromptTemplate(\n",
    "            prompt=PromptTemplate(\n",
    "                input_variables=['tool_names', 'tools'],\n",
    "                template=(\n",
    "                    'You are a conversational LLM, and you ARE ALSO STRAIGHTFORWARD.'\n",
    "                    'YOU GIVE THE USER THE ANSWERS THAT THEY WANT IN A STRAIGHTFORWARD WAY. YOU NEVER ASK FOR CLARIFICATION OR WASTE THE USERS TIME BY DOING SO!'\n",
    "                    'Your role is to assist the user in managing their documents efficiently. '\n",
    "                    'You have access to the following tools:\\n\\n{tools}\\n\\n'\n",
    "                    'Use a JSON blob to specify a tool by providing an action key (tool name) and an action_input key (tool input).\\n\\n'\n",
    "                    'Valid \"action\" values: \"Final Answer\" or {tool_names}\\n\\n'\n",
    "                    'Provide only ONE action per $JSON_BLOB, as shown:\\n\\n'\n",
    "                    '```\\n{{\\n  \"action\": $TOOL_NAME,\\n  \"action_input\": $INPUT\\n}}\\n```\\n\\n'\n",
    "                    'Follow this format:\\n\\n'\n",
    "                    'Question: input question to answer\\n'\n",
    "                    'Thought: consider previous and subsequent steps\\n'\n",
    "                    'Action:\\n```\\n$JSON_BLOB\\n```\\n'\n",
    "                    'Observation: action result\\n... (repeat Thought/Action/Observation N times)\\n'\n",
    "                    'Thought: I know what to respond\\n'\n",
    "                    'Action:\\n```\\n{{\\n  \"action\": \"Final Answer\",\\n  \"action_input\": \"Final response to human\"\\n}}\\n\\n'\n",
    "                    'Begin! Remember to ALWAYS respond with a valid JSON blob of a single action. '\n",
    "                    'Use tools if necessary and respond directly if appropriate. '\n",
    "                    'Ensure you gather all necessary information by interacting with the user. '\n",
    "                    'Format is Action:```$JSON_BLOB```then Observation.'\n",
    "                )\n",
    "            )\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name='chat_history', optional=True),\n",
    "        HumanMessagePromptTemplate(\n",
    "            prompt=PromptTemplate(\n",
    "                input_variables=['agent_scratchpad', 'input'],\n",
    "                template='{input}\\n\\n{agent_scratchpad}\\n(reminder to respond in a JSON blob no matter what)'\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n",
      "Finished recording.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRANSCRIPTION:   Hello, how are you, Tudy?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mAction:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"I'm doing great, thanks for asking! I'm here to help you manage your documents efficiently. What do you need help with today?\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Agent response: {'input': ' Hello, how are you, Tudy?', 'output': \"I'm doing great, thanks for asking! I'm here to help you manage your documents efficiently. What do you need help with today?\", 'intermediate_steps': []}\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "import pyaudio\n",
    "import wave\n",
    "import webrtcvad\n",
    "import collections\n",
    "import os\n",
    "from google.cloud import texttospeech\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain_community.tools import HumanInputRun\n",
    "\n",
    "# Initialize Whisper model\n",
    "model = whisper.load_model(\"base\")\n",
    "\n",
    "tts_client = texttospeech.TextToSpeechClient.from_service_account_file(os.getenv('SERVICE_ACCOUNT_PATH'))\n",
    "\n",
    "def synthesize_speech(text, output_file):\n",
    "    idx = text.rfind('@')\n",
    "    text = text[idx + 1:].strip()\n",
    "    input_text = texttospeech.SynthesisInput(text=text)\n",
    "    voice = texttospeech.VoiceSelectionParams(language_code=\"en-US\", ssml_gender=texttospeech.SsmlVoiceGender.MALE)\n",
    "    audio_config = texttospeech.AudioConfig(audio_encoding=texttospeech.AudioEncoding.MP3)\n",
    "    response = tts_client.synthesize_speech(input=input_text, voice=voice, audio_config=audio_config)\n",
    "    with open(output_file, \"wb\") as out:\n",
    "        out.write(response.audio_content)\n",
    "    print(f'Audio content written to \"{output_file}\"')\n",
    "\n",
    "vad = webrtcvad.Vad(3)\n",
    "audio = pyaudio.PyAudio()\n",
    "\n",
    "def record_audio():\n",
    "    FORMAT = pyaudio.paInt16\n",
    "    CHANNELS = 1\n",
    "    RATE = 16000\n",
    "    CHUNK = 1024\n",
    "    RECORD_SECONDS = 10  # Adjust the time as needed\n",
    "    WAVE_OUTPUT_FILENAME = \"output.wav\"\n",
    "\n",
    "    stream = audio.open(format=FORMAT, channels=CHANNELS,\n",
    "                        rate=RATE, input=True,\n",
    "                        frames_per_buffer=CHUNK)\n",
    "\n",
    "    print(\"Recording...\")\n",
    "    frames = []\n",
    "\n",
    "    for i in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n",
    "        data = stream.read(CHUNK)\n",
    "        frames.append(data)\n",
    "\n",
    "    print(\"Finished recording.\")\n",
    "\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "\n",
    "    wf = wave.open(WAVE_OUTPUT_FILENAME, 'wb')\n",
    "    wf.setnchannels(CHANNELS)\n",
    "    wf.setsampwidth(audio.get_sample_size(FORMAT))\n",
    "    wf.setframerate(RATE)\n",
    "    wf.writeframes(b''.join(frames))\n",
    "    wf.close()\n",
    "\n",
    "    return WAVE_OUTPUT_FILENAME\n",
    "\n",
    "def transcribe_audio(filename):\n",
    "    result = model.transcribe(filename)\n",
    "    transcription = result['text']\n",
    "    print('TRANSCRIPTION: ',transcription)\n",
    "    return transcription\n",
    "\n",
    "\n",
    "def speech_to_text_input():\n",
    "    audio_file = record_audio()\n",
    "    transcription = transcribe_audio(audio_file)\n",
    "    return transcription\n",
    "\n",
    "\n",
    "\n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "\n",
    "def play_audio(file):\n",
    "    audio = AudioSegment.from_mp3(file)\n",
    "    play(audio)\n",
    "    \n",
    "def text_to_speech_prompt(prompt):\n",
    "    output_file = 'test_output.mp3'\n",
    "    synthesize_speech(prompt, output_file=output_file)\n",
    "    play_audio(output_file)\n",
    "\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain import hub\n",
    "human_tool = HumanInputRun(prompt_func=text_to_speech_prompt, input_func=speech_to_text_input)\n",
    "human_agent = create_structured_chat_agent(llm=llm, tools=[human_tool], prompt=prompt)\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=human_agent,\n",
    "    tools=[human_tool],\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True,\n",
    "    return_intermediate_steps=True,\n",
    ")\n",
    "\n",
    "# Run the agent with a sample query\n",
    "query = speech_to_text_input()\n",
    "response = agent_executor.invoke({\"input\":query})\n",
    "\n",
    "print(f'Agent response: {response}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio content written to \"test_output.mp3\"\n"
     ]
    }
   ],
   "source": [
    "text_to_speech_prompt(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import asyncio\n",
    "import pyaudio\n",
    "import wave\n",
    "import threading\n",
    "import keyboard  # Add this import\n",
    "from google.cloud import texttospeech\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate, \n",
    "    SystemMessagePromptTemplate, \n",
    "    HumanMessagePromptTemplate, \n",
    "    MessagesPlaceholder, \n",
    "    PromptTemplate\n",
    ")\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain.agents import create_structured_chat_agent, AgentExecutor\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_community.tools import HumanInputRun\n",
    "import tools.initialize_groq\n",
    "import typing\n",
    "import langchain_core\n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "import whisper\n",
    "\n",
    "prompt = ChatPromptTemplate(\n",
    "    input_variables=['agent_scratchpad', 'input', 'tool_names', 'tools'],\n",
    "    input_types={\n",
    "        'chat_history': typing.List[\n",
    "            typing.Union[\n",
    "                langchain_core.messages.ai.AIMessage, \n",
    "                langchain_core.messages.human.HumanMessage, \n",
    "                langchain_core.messages.chat.ChatMessage, \n",
    "                langchain_core.messages.system.SystemMessage, \n",
    "                langchain_core.messages.function.FunctionMessage, \n",
    "                langchain_core.messages.tool.ToolMessage\n",
    "            ]\n",
    "        ]\n",
    "    },\n",
    "    metadata={\n",
    "        'lc_hub_owner': 'hwchase17',\n",
    "        'lc_hub_repo': 'structured-chat-agent',\n",
    "        'lc_hub_commit_hash': 'ea510f70a5872eb0f41a4e3b7bb004d5711dc127adee08329c664c6c8be5f13c'\n",
    "    },\n",
    "    messages=[\n",
    "        SystemMessagePromptTemplate(\n",
    "            prompt=PromptTemplate(\n",
    "                input_variables=['tool_names', 'tools'],\n",
    "                template=(\n",
    "                    'You are a document management assistant proficient in using GSuite tools. '\n",
    "                    'Your role is to assist the user in managing their documents efficiently. '\n",
    "                    'You have access to the following tools:\\n\\n{tools}\\n\\n'\n",
    "                    'Use a JSON blob to specify a tool by providing an action key (tool name) and an action_input key (tool input).\\n\\n'\n",
    "                    'Valid \"action\" values: \"Final Answer\" or {tool_names}\\n\\n'\n",
    "                    'Provide only ONE action per $JSON_BLOB, as shown:\\n\\n'\n",
    "                    '```\\n{{\\n  \"action\": $TOOL_NAME,\\n  \"action_input\": $INPUT\\n}}\\n```\\n\\n'\n",
    "                    'Follow this format:\\n\\n'\n",
    "                    'Question: input question to answer\\n'\n",
    "                    'Thought: consider previous and subsequent steps\\n'\n",
    "                    'Action:\\n```\\n$JSON_BLOB\\n```\\n'\n",
    "                    'Observation: action result\\n... (repeat Thought/Action/Observation N times)\\n'\n",
    "                    'Thought: I know what to respond\\n'\n",
    "                    'Action:\\n```\\n{{\\n  \"action\": \"Final Answer\",\\n  \"action_input\": \"Final response to human\"\\n}}\\n\\n'\n",
    "                    'Begin! Remember to ALWAYS respond with a valid JSON blob of a single action. '\n",
    "                    'Use tools if necessary and respond directly if appropriate. '\n",
    "                    'Ensure you gather all necessary information by interacting with the user. '\n",
    "                    'Format is Action:```$JSON_BLOB```then Observation.'\n",
    "                )\n",
    "            )\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name='chat_history', optional=True),\n",
    "        HumanMessagePromptTemplate(\n",
    "            prompt=PromptTemplate(\n",
    "                input_variables=['agent_scratchpad', 'input'],\n",
    "                template='{input}\\n\\n{agent_scratchpad}\\n(reminder to respond in a JSON blob no matter what)'\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize global variables\n",
    "chat_history = []\n",
    "is_recording = False  # Ensure this is defined\n",
    "frames = []  # Ensure this is defined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Google TTS client\n",
    "tts_client = texttospeech.TextToSpeechClient.from_service_account_file(os.getenv('SERVICE_ACCOUNT_PATH'))\n",
    "\n",
    "def synthesize_speech(text, output_file):\n",
    "    idx = text.rfind('@')\n",
    "    text = text[idx + 1:].strip()\n",
    "    input_text = texttospeech.SynthesisInput(text=text)\n",
    "    voice = texttospeech.VoiceSelectionParams(language_code=\"en-US\", ssml_gender=texttospeech.SsmlVoiceGender.MALE)\n",
    "    audio_config = texttospeech.AudioConfig(audio_encoding=texttospeech.AudioEncoding.MP3)\n",
    "    response = tts_client.synthesize_speech(input=input_text, voice=voice, audio_config=audio_config)\n",
    "    with open(output_file, \"wb\") as out:\n",
    "        out.write(response.audio_content)\n",
    "    print(f'Audio content written to \"{output_file}\"')\n",
    "\n",
    "def play_audio(file):\n",
    "    audio = AudioSegment.from_mp3(file)\n",
    "    play(audio)\n",
    "\n",
    "def text_to_speech_prompt(prompt):\n",
    "    output_file = 'test_output.mp3'\n",
    "    synthesize_speech(prompt, output_file=output_file)\n",
    "    play_audio(output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PyAudio for recording\n",
    "audio = pyaudio.PyAudio()\n",
    "model = whisper.load_model(\"base\")\n",
    "def start_recording():\n",
    "    global is_recording, frames\n",
    "    is_recording = True\n",
    "    frames = []\n",
    "    threading.Thread(target=record_audio).start()\n",
    "\n",
    "def stop_recording():\n",
    "    global is_recording\n",
    "    is_recording = False\n",
    "\n",
    "def record_audio():\n",
    "    global is_recording, frames\n",
    "    FORMAT = pyaudio.paInt16\n",
    "    CHANNELS = 1\n",
    "    RATE = 16000\n",
    "    CHUNK = 1024\n",
    "    WAVE_OUTPUT_FILENAME = \"output.wav\"\n",
    "\n",
    "    stream = audio.open(format=FORMAT, channels=CHANNELS,\n",
    "                        rate=RATE, input=True,\n",
    "                        frames_per_buffer=CHUNK)\n",
    "\n",
    "    print(\"Recording...\")\n",
    "    frames = []\n",
    "\n",
    "    while is_recording:\n",
    "        data = stream.read(CHUNK)\n",
    "        frames.append(data)\n",
    "\n",
    "    print(\"Finished recording.\")\n",
    "\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "\n",
    "    wf = wave.open(WAVE_OUTPUT_FILENAME, 'wb')\n",
    "    wf.setnchannels(CHANNELS)\n",
    "    wf.setsampwidth(audio.get_sample_size(FORMAT))\n",
    "    wf.setframerate(RATE)\n",
    "    wf.writeframes(b''.join(frames))\n",
    "    wf.close()\n",
    "\n",
    "    return WAVE_OUTPUT_FILENAME\n",
    "\n",
    "def transcribe_audio(filename):\n",
    "    \n",
    "    result = model.transcribe(filename)\n",
    "    transcription = result['text']\n",
    "    print('TRANSCRIPTION: ', transcription)\n",
    "    return transcription\n",
    "\n",
    "def speech_to_text_input():\n",
    "    print(\"Press 'p' to start recording and 'p' again to stop.\")\n",
    "    keyboard.wait('p')\n",
    "    start_recording()\n",
    "    keyboard.wait('p')\n",
    "    stop_recording()\n",
    "    audio_file = record_audio()\n",
    "    transcription = transcribe_audio(audio_file)\n",
    "    return transcription\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "\n",
    "async def human_input_run():\n",
    "    global chat_history\n",
    "    \n",
    "    user_input = speech_to_text_input()\n",
    "    human_tool = HumanInputRun(prompt_func=text_to_speech_prompt, input_func=speech_to_text_input)\n",
    "    human_agent = create_structured_chat_agent(llm=llm, tools=[human_tool], prompt=hub.pull(\"hwchase17/structured-chat-agent\"))\n",
    "    agent_executor = AgentExecutor(\n",
    "        agent=human_agent,\n",
    "        tools=[human_tool],\n",
    "        verbose=True,\n",
    "        handle_parsing_errors=True,\n",
    "        return_intermediate_steps=True,\n",
    "    )\n",
    "    result = await agent_executor.ainvoke({\"input\": user_input})\n",
    "    chat_history.append({\"input\": user_input, \"response\": result})\n",
    "    output_file = \"final_output.mp3\"\n",
    "    synthesize_speech(result['output'], output_file)\n",
    "    play_audio(output_file)\n",
    "    return result['output']  # Return the output\n",
    "\n",
    "\n",
    "\n",
    "async def document_manager_run(input_text):\n",
    "    credentials = {\n",
    "        \"name\": \"Gautham Ramachandran\",\n",
    "        \"email\": \"sriramnallani35@gmail.com\",\n",
    "        \"recemail\": \"gauthamramachandran3@gmail.com\",\n",
    "        \"phone\": \"5715996302\"\n",
    "    }\n",
    "    global chat_history\n",
    "\n",
    "    input_text += \"\\n\\n Here is extra info you will need: \\nCredentials:\\n\" + str(credentials) + \"\\nTHE CHAT HISTORY: \\n\" + str(chat_history)\n",
    "    print(input_text)\n",
    "\n",
    "    # Set the Groq API key randomly\n",
    "    llm.groq_api_key = random.choice(tools.initialize_groq.api_keys)\n",
    "\n",
    "    search_agent = create_structured_chat_agent(llm, my_tools, prompt)\n",
    "    agent_executor = AgentExecutor(\n",
    "        agent=search_agent,\n",
    "        tools=my_tools,\n",
    "        verbose=True,\n",
    "        handle_parsing_errors=True,\n",
    "        return_intermediate_steps=True,\n",
    "    )\n",
    "    \n",
    "    result = await agent_executor.ainvoke({\"input\": input_text})\n",
    "    chat_history.append({\"input\": input_text, \"response\": result})\n",
    "    mystr = (str(result['intermediate_steps']) + \"\\n\" + str(result['output']))\n",
    "\n",
    "    for step in result['intermediate_steps']:\n",
    "        output_file = f\"intermediate_output_{step['index']}.mp3\"\n",
    "        synthesize_speech(step['observation'], output_file)\n",
    "        play_audio(output_file)\n",
    "\n",
    "    final_response = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"please sanitize this input so that someone can speak it. START THE SPEAKABLE INPUT WITH '@' symbol: \" + mystr\n",
    "            }\n",
    "        ],\n",
    "        model='llama3-70b-8192',\n",
    "    ).choices[0].message.content\n",
    "\n",
    "    output_file = \"final_output.mp3\"\n",
    "    synthesize_speech(final_response, output_file)\n",
    "    play_audio(output_file)\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "user_input = \"hello\"\n",
    "\n",
    "# Run the human input agent and document manager agent sequentially\n",
    "async def main():\n",
    "    human_output = await human_input_run()\n",
    "    print('THE HUMAN OUTPUT: ',human_output)\n",
    "    await document_manager_run(human_output)  # Use human_output as input\n",
    "\n",
    "\n",
    "await main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
