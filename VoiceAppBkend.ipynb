{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install flask whisper\n",
    "#! pip install git+https://github.com/openai/whisper.git -q\n",
    "#!pip install ffmpeg\n",
    "#!pip install openai\n",
    "#!pip install transformers\n",
    "#!pip install librosa pyaudio\n",
    "#!pip install --force-reinstall scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-03 21:59:51,258 - INFO - \u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on http://127.0.0.1:5000\n",
      "2024-05-03 21:59:51,258 - INFO - \u001b[33mPress CTRL+C to quit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-03 22:00:05,720 - ERROR - An error occurred during transcription\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\PC-User\\AppData\\Local\\Temp\\ipykernel_23388\\3213918725.py\", line 250, in transcribe_audio\n",
      "    result = model.transcribe(temp_audio_path)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\whisper\\transcribe.py\", line 279, in transcribe\n",
      "    result: DecodingResult = decode_with_fallback(mel_segment)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\whisper\\transcribe.py\", line 195, in decode_with_fallback\n",
      "    decode_result = model.decode(segment, options)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\whisper\\decoding.py\", line 824, in decode\n",
      "    result = DecodingTask(model, options).run(mel)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\whisper\\decoding.py\", line 737, in run\n",
      "    tokens, sum_logprobs, no_speech_probs = self._main_loop(audio_features, tokens)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\whisper\\decoding.py\", line 687, in _main_loop\n",
      "    logits = self.inference.logits(tokens, audio_features)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\whisper\\decoding.py\", line 163, in logits\n",
      "    return self.model.decoder(tokens, audio_features, kv_cache=self.kv_cache)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\whisper\\model.py\", line 211, in forward\n",
      "    x = block(x, xa, mask=self.mask, kv_cache=kv_cache)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\whisper\\model.py\", line 136, in forward\n",
      "    x = x + self.attn(self.attn_ln(x), mask=mask, kv_cache=kv_cache)[0]\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\whisper\\model.py\", line 90, in forward\n",
      "    wv, qk = self.qkv_attention(q, k, v, mask)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\whisper\\model.py\", line 104, in qkv_attention\n",
      "    qk = qk + mask[:n_ctx, :n_ctx]\n",
      "RuntimeError: The size of tensor a (6) must match the size of tensor b (3) at non-singleton dimension 3\n",
      "2024-05-03 22:00:07,207 - ERROR - An error occurred during transcription\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\PC-User\\AppData\\Local\\Temp\\ipykernel_23388\\3213918725.py\", line 250, in transcribe_audio\n",
      "    result = model.transcribe(temp_audio_path)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\whisper\\transcribe.py\", line 279, in transcribe\n",
      "    result: DecodingResult = decode_with_fallback(mel_segment)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\whisper\\transcribe.py\", line 195, in decode_with_fallback\n",
      "    decode_result = model.decode(segment, options)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\whisper\\decoding.py\", line 824, in decode\n",
      "    result = DecodingTask(model, options).run(mel)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\whisper\\decoding.py\", line 737, in run\n",
      "    tokens, sum_logprobs, no_speech_probs = self._main_loop(audio_features, tokens)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\whisper\\decoding.py\", line 687, in _main_loop\n",
      "    logits = self.inference.logits(tokens, audio_features)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\whisper\\decoding.py\", line 163, in logits\n",
      "    return self.model.decoder(tokens, audio_features, kv_cache=self.kv_cache)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\whisper\\model.py\", line 211, in forward\n",
      "    x = block(x, xa, mask=self.mask, kv_cache=kv_cache)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\whisper\\model.py\", line 136, in forward\n",
      "    x = x + self.attn(self.attn_ln(x), mask=mask, kv_cache=kv_cache)[0]\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\whisper\\model.py\", line 90, in forward\n",
      "    wv, qk = self.qkv_attention(q, k, v, mask)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\whisper\\model.py\", line 104, in qkv_attention\n",
      "    qk = qk + mask[:n_ctx, :n_ctx]\n",
      "RuntimeError: The size of tensor a (14) must match the size of tensor b (3) at non-singleton dimension 3\n",
      "2024-05-03 22:00:07,300 - ERROR - An error occurred during transcription\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\PC-User\\AppData\\Local\\Temp\\ipykernel_23388\\3213918725.py\", line 250, in transcribe_audio\n",
      "    result = model.transcribe(temp_audio_path)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\whisper\\transcribe.py\", line 279, in transcribe\n",
      "    result: DecodingResult = decode_with_fallback(mel_segment)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\whisper\\transcribe.py\", line 195, in decode_with_fallback\n",
      "    decode_result = model.decode(segment, options)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\whisper\\decoding.py\", line 824, in decode\n",
      "    result = DecodingTask(model, options).run(mel)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\whisper\\decoding.py\", line 737, in run\n",
      "    tokens, sum_logprobs, no_speech_probs = self._main_loop(audio_features, tokens)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\whisper\\decoding.py\", line 687, in _main_loop\n",
      "    logits = self.inference.logits(tokens, audio_features)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\whisper\\decoding.py\", line 163, in logits\n",
      "    return self.model.decoder(tokens, audio_features, kv_cache=self.kv_cache)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\whisper\\model.py\", line 211, in forward\n",
      "    x = block(x, xa, mask=self.mask, kv_cache=kv_cache)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\whisper\\model.py\", line 136, in forward\n",
      "    x = x + self.attn(self.attn_ln(x), mask=mask, kv_cache=kv_cache)[0]\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\whisper\\model.py\", line 90, in forward\n",
      "    wv, qk = self.qkv_attention(q, k, v, mask)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\whisper\\model.py\", line 104, in qkv_attention\n",
      "    qk = qk + mask[:n_ctx, :n_ctx]\n",
      "RuntimeError: The size of tensor a (17) must match the size of tensor b (3) at non-singleton dimension 3\n",
      "2024-05-03 22:00:07,309 - ERROR - An error occurred during transcription\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\PC-User\\AppData\\Local\\Temp\\ipykernel_23388\\3213918725.py\", line 250, in transcribe_audio\n",
      "    result = model.transcribe(temp_audio_path)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\whisper\\transcribe.py\", line 279, in transcribe\n",
      "    result: DecodingResult = decode_with_fallback(mel_segment)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\whisper\\transcribe.py\", line 195, in decode_with_fallback\n",
      "    decode_result = model.decode(segment, options)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\whisper\\decoding.py\", line 824, in decode\n",
      "    result = DecodingTask(model, options).run(mel)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\whisper\\decoding.py\", line 737, in run\n",
      "    tokens, sum_logprobs, no_speech_probs = self._main_loop(audio_features, tokens)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\whisper\\decoding.py\", line 687, in _main_loop\n",
      "    logits = self.inference.logits(tokens, audio_features)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\whisper\\decoding.py\", line 163, in logits\n",
      "    return self.model.decoder(tokens, audio_features, kv_cache=self.kv_cache)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\whisper\\model.py\", line 211, in forward\n",
      "    x = block(x, xa, mask=self.mask, kv_cache=kv_cache)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\whisper\\model.py\", line 136, in forward\n",
      "    x = x + self.attn(self.attn_ln(x), mask=mask, kv_cache=kv_cache)[0]\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\whisper\\model.py\", line 90, in forward\n",
      "    wv, qk = self.qkv_attention(q, k, v, mask)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\whisper\\model.py\", line 104, in qkv_attention\n",
      "    qk = qk + mask[:n_ctx, :n_ctx]\n",
      "RuntimeError: The size of tensor a (20) must match the size of tensor b (3) at non-singleton dimension 3\n",
      "2024-05-03 22:00:15,730 - ERROR - Exception on /transcribe [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\flask\\app.py\", line 1473, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\flask\\app.py\", line 883, in full_dispatch_request\n",
      "    return self.finalize_request(rv)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\flask\\app.py\", line 902, in finalize_request\n",
      "    response = self.make_response(rv)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\flask\\app.py\", line 1174, in make_response\n",
      "    raise TypeError(\n",
      "TypeError: The view function for 'transcribe_audio' did not return a valid response. The function either returned None or ended without a return statement.\n",
      "2024-05-03 22:00:15,731 - INFO - 127.0.0.1 - - [03/May/2024 22:00:15] \"\u001b[35m\u001b[1mPOST /transcribe HTTP/1.1\u001b[0m\" 500 -\n",
      "2024-05-03 22:00:17,230 - ERROR - Exception on /transcribe [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\flask\\app.py\", line 1473, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\flask\\app.py\", line 883, in full_dispatch_request\n",
      "    return self.finalize_request(rv)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\flask\\app.py\", line 902, in finalize_request\n",
      "    response = self.make_response(rv)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\flask\\app.py\", line 1174, in make_response\n",
      "    raise TypeError(\n",
      "TypeError: The view function for 'transcribe_audio' did not return a valid response. The function either returned None or ended without a return statement.\n",
      "2024-05-03 22:00:17,230 - INFO - 127.0.0.1 - - [03/May/2024 22:00:17] \"\u001b[35m\u001b[1mPOST /transcribe HTTP/1.1\u001b[0m\" 500 -\n",
      "2024-05-03 22:00:17,313 - ERROR - Exception on /transcribe [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\flask\\app.py\", line 1473, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\flask\\app.py\", line 883, in full_dispatch_request\n",
      "    return self.finalize_request(rv)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\flask\\app.py\", line 902, in finalize_request\n",
      "    response = self.make_response(rv)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\flask\\app.py\", line 1174, in make_response\n",
      "    raise TypeError(\n",
      "TypeError: The view function for 'transcribe_audio' did not return a valid response. The function either returned None or ended without a return statement.\n",
      "2024-05-03 22:00:17,314 - ERROR - Exception on /transcribe [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\flask\\app.py\", line 1473, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\flask\\app.py\", line 883, in full_dispatch_request\n",
      "    return self.finalize_request(rv)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\flask\\app.py\", line 902, in finalize_request\n",
      "    response = self.make_response(rv)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\flask\\app.py\", line 1174, in make_response\n",
      "    raise TypeError(\n",
      "TypeError: The view function for 'transcribe_audio' did not return a valid response. The function either returned None or ended without a return statement.\n",
      "2024-05-03 22:00:17,316 - INFO - 127.0.0.1 - - [03/May/2024 22:00:17] \"\u001b[35m\u001b[1mPOST /transcribe HTTP/1.1\u001b[0m\" 500 -\n",
      "2024-05-03 22:00:17,318 - INFO - 127.0.0.1 - - [03/May/2024 22:00:17] \"\u001b[35m\u001b[1mPOST /transcribe HTTP/1.1\u001b[0m\" 500 -\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import time\n",
    "from flask import Flask, request, jsonify\n",
    "import tempfile\n",
    "import whisper\n",
    "import numpy as np\n",
    "import os\n",
    "import librosa \n",
    "import pyaudio\n",
    "# Setup Flask app\n",
    "app = Flask(__name__)\n",
    " # Allow cross-origin requests\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Load the Whisper model once to save resources\n",
    "model = whisper.load_model(\"base\")\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return \"\"\"<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Real-Time Speech to Text</title>\n",
    "    <style>\n",
    "        /* Add Skype-like styling */\n",
    "        body {\n",
    "            font-family: Arial, sans-serif;\n",
    "            background-color: #fff;\n",
    "        }\n",
    "        .container {\n",
    "            max-width: 400px;\n",
    "            margin: 40px auto;\n",
    "            background-color: #f7f7f7;\n",
    "            padding: 20px;\n",
    "            border-radius: 10px;\n",
    "            box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);\n",
    "        }\n",
    "        .header {\n",
    "            background-color: #333;\n",
    "            color: #fff;\n",
    "            padding: 10px;\n",
    "            border-bottom: 1px solid #333;\n",
    "            border-radius: 10px 10px 0 0;\n",
    "        }\n",
    "        .header span {\n",
    "            font-weight: bold;\n",
    "            font-size: 18px;\n",
    "        }\n",
    "        .call-info {\n",
    "            margin-top: 20px;\n",
    "            display: flex;\n",
    "            flex-wrap: wrap;\n",
    "            justify-content: space-between;\n",
    "        }\n",
    "        .call-info span {\n",
    "            font-weight: bold;\n",
    "            margin-right: 10px;\n",
    "            font-size: 16px;\n",
    "        }\n",
    "        .transcription {\n",
    "            margin-top: 20px;\n",
    "            padding: 20px;\n",
    "            border: 1px solid #ddd;\n",
    "            border-radius: 10px;\n",
    "            background-color: #f9f9f9;\n",
    "        }\n",
    "        .button-container {\n",
    "            margin-top: 20px;\n",
    "            text-align: center;\n",
    "        }\n",
    "        .button-container button {\n",
    "            padding: 10px 20px;\n",
    "            border: none;\n",
    "            border-radius: 10px;\n",
    "            background-color: #333;\n",
    "            color: #fff;\n",
    "            cursor: pointer;\n",
    "        }\n",
    "        .button-container button:hover {\n",
    "            background-color: #444;\n",
    "        }\n",
    "        /* Add Skype-like video call elements */\n",
    "        .video-call {\n",
    "            display: flex;\n",
    "            flex-wrap: wrap;\n",
    "            justify-content: center;\n",
    "            margin-top: 20px;\n",
    "        }\n",
    "        .video-call div {\n",
    "            width: 50%;\n",
    "            height: 200px;\n",
    "            background-color: #333;\n",
    "            border-radius: 10px;\n",
    "            margin: 10px;\n",
    "        }\n",
    "        .video-call div:first-child {\n",
    "            background-image: url('/static/Images/man1_livecall.jfif');\n",
    "            background-size: cover;\n",
    "            background-position: center;\n",
    "        }\n",
    "        .video-call div:last-child {\n",
    "            background-image: url('/static/Images/man2_livecall.jfif');\n",
    "            background-size: cover;\n",
    "            background-position: center;\n",
    "        }\n",
    "\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"container\">\n",
    "        <div class=\"header\">\n",
    "            <span>Skype Call</span>\n",
    "        </div>\n",
    "        <div class=\"call-info\">\n",
    "            <span>Call ID:</span> <span>1234567890</span>\n",
    "            <span>Caller:</span> <span>John Doe</span>\n",
    "            <span>Duration:</span> <span>00:00:00</span>\n",
    "        </div>\n",
    "        <div class=\"video-call\">\n",
    "            <div></div>\n",
    "            <div></div>\n",
    "        </div>\n",
    "        <div class=\"transcription\">\n",
    "            <p id=\"transcription\"></p>\n",
    "        </div>\n",
    "        <div class=\"button-container\">\n",
    "            <button id=\"endCall\">End Call</button>\n",
    "        </div>\n",
    "    </div>\n",
    "\n",
    "    <script>\n",
    "    let audioContext, microphone, mediaRecorder, audioChunks = [];\n",
    "let lastSoundTimestamp = Date.now();\n",
    "let chunking=4000;\n",
    "const silenceThreshold = 2000; // Time in milliseconds to define silence duration\n",
    "\n",
    "navigator.mediaDevices.getUserMedia({ audio: true })\n",
    "    .then(stream => {\n",
    "        audioContext = new AudioContext();\n",
    "        microphone = audioContext.createMediaStreamSource(stream);\n",
    "        mediaRecorder = new MediaRecorder(stream);\n",
    "\n",
    "        // Event fired when audio data is available\n",
    "        mediaRecorder.ondataavailable = event => {\n",
    "            if (event.data.size > 0) {\n",
    "                console.log(\"Audio incoming. Audio incoming. Chunk pushed\");\n",
    "                audioChunks.push(event.data);\n",
    "            }\n",
    "        };\n",
    "\n",
    "        mediaRecorder.start(chunking); \n",
    "\n",
    "        // Setup a ScriptProcessorNode to detect silence\n",
    "        const processor = audioContext.createScriptProcessor(2048, 1, 1);\n",
    "        microphone.connect(processor);\n",
    "        processor.connect(audioContext.destination);\n",
    "\n",
    "        processor.onaudioprocess = function(event) {\n",
    "            var input = event.inputBuffer.getChannelData(0);\n",
    "            var sum = 0;\n",
    "            for (var i = 0; i < input.length; ++i) {\n",
    "                sum += input[i] * input[i];\n",
    "            }\n",
    "            var rms = Math.sqrt(sum / input.length);\n",
    "            console.log(\"RMS = \" + rms);\n",
    "            if (rms >= 0.008) {\n",
    "                lastSoundTimestamp = Date.now();\n",
    "            } else if ((Date.now() - lastSoundTimestamp) > silenceThreshold && audioChunks.length) {\n",
    "                // Detected silence, stop the recorder and send data\n",
    "                console.log(\"Detected silence, stop the recorder and send data & audioChunks list length \" + audioChunks.length);\n",
    "                mediaRecorder.stop();\n",
    "            }\n",
    "        };\n",
    "\n",
    "        // Restart recorder after sending data\n",
    "        mediaRecorder.onstop = () => {\n",
    "            sendAudioToServer();\n",
    "            audioChunks = []; // Clear the buffer after sending\n",
    "            mediaRecorder.start(chunking); // Restart recording after processing\n",
    "        };\n",
    "    })\n",
    "    .catch(error => console.error('Error accessing media devices.', error));\n",
    "\n",
    "function sendAudioToServer() {\n",
    "    const audioBlob = new Blob(audioChunks, { type: 'audio/wav' });\n",
    "    const formData = new FormData();\n",
    "    formData.append('audio', audioBlob, 'file.wav');\n",
    "\n",
    "    fetch('/transcribe', {\n",
    "        method: 'POST',\n",
    "        body: formData\n",
    "    })\n",
    "    .then(response => response.json())\n",
    "    .then(data => {\n",
    "        document.getElementById('transcription').textContent = data.transcription;\n",
    "        console.log(\"Transcription:\", data.transcription);\n",
    "    })\n",
    "    .catch(console.error);\n",
    "}\n",
    "\n",
    "\n",
    "    document.getElementById('endCall').addEventListener('click', () => {\n",
    "        window.reload();\n",
    "    });\n",
    "</script>\n",
    "\n",
    "\n",
    "</body>\n",
    "</html>\n",
    "\n",
    "\n",
    "            \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def format_list(lst:list):\n",
    "    string = ''\n",
    "    ctr = 0\n",
    "    for c in lst:\n",
    "        string += (c + \" \")\n",
    "        if(ctr > 6):\n",
    "            string += (\"\\n\")\n",
    "            ctr = 0\n",
    "        ctr += 1\n",
    "    return string\n",
    "\n",
    "conversation_history = []\n",
    "@app.route('/transcribe', methods=['POST'])\n",
    "def transcribe_audio():\n",
    "    global conversation_history\n",
    "    \n",
    "\n",
    "    audio_file = request.files['audio']\n",
    "    \n",
    "    temp_dir = tempfile.mkdtemp(dir=\"D:\\\\DEV\\\\WebdevFolder\\\\RealEstateAI\")\n",
    "    try:\n",
    "        # Save the audio file to a temporary file\n",
    "        temp_audio_path = os.path.join(temp_dir, audio_file.filename)\n",
    "        audio_file.save(temp_audio_path)\n",
    "\n",
    "       \n",
    "        result = model.transcribe(temp_audio_path)\n",
    "        \n",
    "        conversation_history.append( result['text'])\n",
    "        conversation_history_str = format_list(conversation_history)\n",
    "        return jsonify({\"transcription\": conversation_history_str})\n",
    "    except Exception as e:\n",
    "        logging.exception(\"An error occurred during transcription\")\n",
    "        \n",
    "    finally:\n",
    "        time.sleep(10)\n",
    "        # Cleanup: Remove temporary files\n",
    "        os.remove(temp_audio_path)\n",
    "        os.rmdir(temp_dir)\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=False, port=5000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
