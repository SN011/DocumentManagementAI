{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install flask whisper\n",
    "#! pip install git+https://github.com/openai/whisper.git -q\n",
    "#!pip install ffmpeg\n",
    "#!pip install openai\n",
    "#!pip install transformers\n",
    "#!pip install librosa pyaudio\n",
    "#!pip install --force-reinstall scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-03 18:55:23,184 - INFO - \u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on http://127.0.0.1:5000\n",
      "2024-05-03 18:55:23,185 - INFO - \u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "2024-05-03 18:55:26,405 - INFO - 127.0.0.1 - - [03/May/2024 18:55:26] \"GET / HTTP/1.1\" 200 -\n",
      "2024-05-03 18:55:26,435 - INFO - 127.0.0.1 - - [03/May/2024 18:55:26] \"\u001b[36mGET /static/Images/man1_livecall.jpg HTTP/1.1\u001b[0m\" 304 -\n",
      "2024-05-03 18:55:26,439 - INFO - 127.0.0.1 - - [03/May/2024 18:55:26] \"\u001b[36mGET /static/Images/man2_livecall.jpg HTTP/1.1\u001b[0m\" 304 -\n",
      "2024-05-03 18:55:43,774 - INFO - 127.0.0.1 - - [03/May/2024 18:55:43] \"POST /transcribe HTTP/1.1\" 200 -\n",
      "2024-05-03 18:55:48,988 - INFO - 127.0.0.1 - - [03/May/2024 18:55:48] \"POST /transcribe HTTP/1.1\" 200 -\n",
      "2024-05-03 18:55:59,169 - INFO - 127.0.0.1 - - [03/May/2024 18:55:59] \"POST /transcribe HTTP/1.1\" 200 -\n",
      "2024-05-03 18:56:35,511 - INFO - 127.0.0.1 - - [03/May/2024 18:56:35] \"POST /transcribe HTTP/1.1\" 200 -\n",
      "2024-05-03 18:56:36,724 - INFO - 127.0.0.1 - - [03/May/2024 18:56:36] \"POST /transcribe HTTP/1.1\" 200 -\n",
      "2024-05-03 18:56:40,339 - INFO - 127.0.0.1 - - [03/May/2024 18:56:40] \"POST /transcribe HTTP/1.1\" 200 -\n",
      "2024-05-03 18:56:44,404 - INFO - 127.0.0.1 - - [03/May/2024 18:56:44] \"POST /transcribe HTTP/1.1\" 200 -\n",
      "2024-05-03 18:56:48,457 - INFO - 127.0.0.1 - - [03/May/2024 18:56:48] \"POST /transcribe HTTP/1.1\" 200 -\n",
      "2024-05-03 18:56:52,650 - INFO - 127.0.0.1 - - [03/May/2024 18:56:52] \"POST /transcribe HTTP/1.1\" 200 -\n",
      "2024-05-03 18:57:02,253 - INFO - 127.0.0.1 - - [03/May/2024 18:57:02] \"POST /transcribe HTTP/1.1\" 200 -\n",
      "2024-05-03 18:57:12,264 - INFO - 127.0.0.1 - - [03/May/2024 18:57:12] \"POST /transcribe HTTP/1.1\" 200 -\n",
      "2024-05-03 18:58:25,886 - INFO - 127.0.0.1 - - [03/May/2024 18:58:25] \"POST /transcribe HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import time\n",
    "from flask import Flask, request, jsonify\n",
    "import tempfile\n",
    "import whisper\n",
    "import numpy as np\n",
    "import os\n",
    "import librosa \n",
    "import pyaudio\n",
    "# Setup Flask app\n",
    "app = Flask(__name__)\n",
    " # Allow cross-origin requests\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Load the Whisper model once to save resources\n",
    "model = whisper.load_model(\"base\")\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return \"\"\"<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Real-Time Speech to Text</title>\n",
    "    <style>\n",
    "        /* Add Skype-like styling */\n",
    "        body {\n",
    "            font-family: Arial, sans-serif;\n",
    "            background-color: #fff;\n",
    "        }\n",
    "        .container {\n",
    "            max-width: 400px;\n",
    "            margin: 40px auto;\n",
    "            background-color: #f7f7f7;\n",
    "            padding: 20px;\n",
    "            border-radius: 10px;\n",
    "            box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);\n",
    "        }\n",
    "        .header {\n",
    "            background-color: #333;\n",
    "            color: #fff;\n",
    "            padding: 10px;\n",
    "            border-bottom: 1px solid #333;\n",
    "            border-radius: 10px 10px 0 0;\n",
    "        }\n",
    "        .header span {\n",
    "            font-weight: bold;\n",
    "            font-size: 18px;\n",
    "        }\n",
    "        .call-info {\n",
    "            margin-top: 20px;\n",
    "            display: flex;\n",
    "            flex-wrap: wrap;\n",
    "            justify-content: space-between;\n",
    "        }\n",
    "        .call-info span {\n",
    "            font-weight: bold;\n",
    "            margin-right: 10px;\n",
    "            font-size: 16px;\n",
    "        }\n",
    "        .transcription {\n",
    "            margin-top: 20px;\n",
    "            padding: 20px;\n",
    "            border: 1px solid #ddd;\n",
    "            border-radius: 10px;\n",
    "            background-color: #f9f9f9;\n",
    "        }\n",
    "        .button-container {\n",
    "            margin-top: 20px;\n",
    "            text-align: center;\n",
    "        }\n",
    "        .button-container button {\n",
    "            padding: 10px 20px;\n",
    "            border: none;\n",
    "            border-radius: 10px;\n",
    "            background-color: #333;\n",
    "            color: #fff;\n",
    "            cursor: pointer;\n",
    "        }\n",
    "        .button-container button:hover {\n",
    "            background-color: #444;\n",
    "        }\n",
    "        /* Add Skype-like video call elements */\n",
    "        .video-call {\n",
    "            display: flex;\n",
    "            flex-wrap: wrap;\n",
    "            justify-content: center;\n",
    "            margin-top: 20px;\n",
    "        }\n",
    "        .video-call div {\n",
    "            width: 50%;\n",
    "            height: 200px;\n",
    "            background-color: #333;\n",
    "            border-radius: 10px;\n",
    "            margin: 10px;\n",
    "        }\n",
    "        .video-call div:first-child {\n",
    "            background-image: url('/static/Images/man1_livecall.jpg');\n",
    "            background-size: cover;\n",
    "            background-position: center;\n",
    "        }\n",
    "        .video-call div:last-child {\n",
    "            background-image: url('/static/Images/man2_livecall.jpg');\n",
    "            background-size: cover;\n",
    "            background-position: center;\n",
    "        }\n",
    "\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"container\">\n",
    "        <div class=\"header\">\n",
    "            <span>Skype Call</span>\n",
    "        </div>\n",
    "        <div class=\"call-info\">\n",
    "            <span>Call ID:</span> <span>1234567890</span>\n",
    "            <span>Caller:</span> <span>John Doe</span>\n",
    "            <span>Duration:</span> <span>00:00:00</span>\n",
    "        </div>\n",
    "        <div class=\"video-call\">\n",
    "            <div></div>\n",
    "            <div></div>\n",
    "        </div>\n",
    "        <div class=\"transcription\">\n",
    "            <p id=\"transcription\"></p>\n",
    "        </div>\n",
    "        <div class=\"button-container\">\n",
    "            <button id=\"endCall\">End Call</button>\n",
    "        </div>\n",
    "    </div>\n",
    "\n",
    "    <script>\n",
    "    let audioContext, microphone, mediaRecorder, audioChunks = [];\n",
    "let lastSoundTimestamp = Date.now();\n",
    "let chunking=4000;\n",
    "const silenceThreshold = 2000; // Time in milliseconds to define silence duration\n",
    "\n",
    "navigator.mediaDevices.getUserMedia({ audio: true })\n",
    "    .then(stream => {\n",
    "        audioContext = new AudioContext();\n",
    "        microphone = audioContext.createMediaStreamSource(stream);\n",
    "        mediaRecorder = new MediaRecorder(stream);\n",
    "\n",
    "        // Event fired when audio data is available\n",
    "        mediaRecorder.ondataavailable = event => {\n",
    "            if (event.data.size > 0) {\n",
    "                audioChunks.push(event.data);\n",
    "            }\n",
    "        };\n",
    "\n",
    "        mediaRecorder.start(chunking); // Collect data in chunks of 1 second\n",
    "\n",
    "        // Setup a ScriptProcessorNode to detect silence\n",
    "        const processor = audioContext.createScriptProcessor(2048, 1, 1);\n",
    "        microphone.connect(processor);\n",
    "        processor.connect(audioContext.destination);\n",
    "\n",
    "        processor.onaudioprocess = function(event) {\n",
    "            var input = event.inputBuffer.getChannelData(0);\n",
    "            var sum = 0;\n",
    "            for (var i = 0; i < input.length; ++i) {\n",
    "                sum += input[i] * input[i];\n",
    "            }\n",
    "            var rms = Math.sqrt(sum / input.length);\n",
    "            if (rms >= 0.01) {\n",
    "                lastSoundTimestamp = Date.now();\n",
    "            } else if ((Date.now() - lastSoundTimestamp) > silenceThreshold && audioChunks.length) {\n",
    "                // Detected silence, stop the recorder and send data\n",
    "                mediaRecorder.stop();\n",
    "            }\n",
    "        };\n",
    "\n",
    "        // Restart recorder after sending data\n",
    "        mediaRecorder.onstop = () => {\n",
    "            sendAudioToServer();\n",
    "            audioChunks = []; // Clear the buffer after sending\n",
    "            mediaRecorder.start(chunking); // Restart recording after processing\n",
    "        };\n",
    "    })\n",
    "    .catch(error => console.error('Error accessing media devices.', error));\n",
    "\n",
    "function sendAudioToServer() {\n",
    "    const audioBlob = new Blob(audioChunks, { type: 'audio/wav' });\n",
    "    const formData = new FormData();\n",
    "    formData.append('audio', audioBlob, 'file.wav');\n",
    "\n",
    "    fetch('/transcribe', {\n",
    "        method: 'POST',\n",
    "        body: formData\n",
    "    })\n",
    "    .then(response => response.json())\n",
    "    .then(data => {\n",
    "        document.getElementById('transcription').textContent = data.transcription;\n",
    "        console.log(\"Transcription:\", data.transcription);\n",
    "    })\n",
    "    .catch(console.error);\n",
    "}\n",
    "\n",
    "\n",
    "    document.getElementById('endCall').addEventListener('click', () => {\n",
    "        window.reload();\n",
    "    });\n",
    "</script>\n",
    "\n",
    "\n",
    "</body>\n",
    "</html>\n",
    "\n",
    "\n",
    "            \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def format_list(lst:list):\n",
    "    string = ''\n",
    "    ctr = 0\n",
    "    for c in lst:\n",
    "        string += (c + \" \")\n",
    "        if(ctr > 6):\n",
    "            string += (\"\\n\")\n",
    "            ctr = 0\n",
    "        ctr += 1\n",
    "    return string\n",
    "\n",
    "conversation_history = []\n",
    "@app.route('/transcribe', methods=['POST'])\n",
    "def transcribe_audio():\n",
    "    global conversation_history\n",
    "    \n",
    "\n",
    "    audio_file = request.files['audio']\n",
    "    \n",
    "    temp_dir = tempfile.mkdtemp(dir=\"D:\\\\DEV\\\\WebdevFolder\\\\RealEstateAI\")\n",
    "    try:\n",
    "        # Save the audio file to a temporary file\n",
    "        temp_audio_path = os.path.join(temp_dir, audio_file.filename)\n",
    "        audio_file.save(temp_audio_path)\n",
    "\n",
    "       \n",
    "        result = model.transcribe(temp_audio_path)\n",
    "        \n",
    "        conversation_history.append( result['text'])\n",
    "        conversation_history_str = format_list(conversation_history)\n",
    "        return jsonify({\"transcription\": conversation_history_str})\n",
    "    except Exception as e:\n",
    "        logging.exception(\"An error occurred during transcription\")\n",
    "        \n",
    "    finally:\n",
    "        time.sleep(10)\n",
    "        # Cleanup: Remove temporary files\n",
    "        os.remove(temp_audio_path)\n",
    "        os.rmdir(temp_dir)\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=False, port=5000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
