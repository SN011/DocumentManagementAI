{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install flask whisper\n",
    "#! pip install git+https://github.com/openai/whisper.git -q\n",
    "#!pip install ffmpeg\n",
    "#!pip install openai\n",
    "#!pip install transformers\n",
    "#!pip install librosa pyaudio\n",
    "#!pip install --force-reinstall scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-03 22:37:41,864 - INFO - \u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on http://127.0.0.1:5000\n",
      "2024-05-03 22:37:41,864 - INFO - \u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "2024-05-03 22:37:48,985 - INFO - 127.0.0.1 - - [03/May/2024 22:37:48] \"GET / HTTP/1.1\" 200 -\n",
      "2024-05-03 22:37:49,016 - INFO - 127.0.0.1 - - [03/May/2024 22:37:49] \"\u001b[36mGET /static/Images/man1_livecall.jfif HTTP/1.1\u001b[0m\" 304 -\n",
      "2024-05-03 22:37:49,021 - INFO - 127.0.0.1 - - [03/May/2024 22:37:49] \"\u001b[36mGET /static/Images/man2_livecall.jfif HTTP/1.1\u001b[0m\" 304 -\n",
      "2024-05-03 22:38:02,634 - INFO - 127.0.0.1 - - [03/May/2024 22:38:02] \"POST /transcribe HTTP/1.1\" 200 -\n",
      "2024-05-03 22:38:22,279 - INFO - 127.0.0.1 - - [03/May/2024 22:38:22] \"POST /transcribe HTTP/1.1\" 200 -\n",
      "2024-05-03 22:38:24,776 - INFO - 127.0.0.1 - - [03/May/2024 22:38:24] \"POST /transcribe HTTP/1.1\" 200 -\n",
      "2024-05-03 22:38:32,246 - INFO - 127.0.0.1 - - [03/May/2024 22:38:32] \"POST /transcribe HTTP/1.1\" 200 -\n",
      "2024-05-03 22:38:41,884 - INFO - 127.0.0.1 - - [03/May/2024 22:38:41] \"POST /transcribe HTTP/1.1\" 200 -\n",
      "2024-05-03 22:38:47,069 - INFO - 127.0.0.1 - - [03/May/2024 22:38:47] \"POST /transcribe HTTP/1.1\" 200 -\n",
      "2024-05-03 22:38:54,170 - INFO - 127.0.0.1 - - [03/May/2024 22:38:54] \"POST /transcribe HTTP/1.1\" 200 -\n",
      "2024-05-03 22:38:56,009 - INFO - 127.0.0.1 - - [03/May/2024 22:38:56] \"POST /transcribe HTTP/1.1\" 200 -\n",
      "2024-05-03 22:39:01,387 - INFO - 127.0.0.1 - - [03/May/2024 22:39:01] \"POST /transcribe HTTP/1.1\" 200 -\n",
      "2024-05-03 22:39:05,508 - INFO - 127.0.0.1 - - [03/May/2024 22:39:05] \"POST /transcribe HTTP/1.1\" 200 -\n",
      "2024-05-03 22:39:30,761 - ERROR - An error occurred during transcription\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\PC-User\\AppData\\Local\\Temp\\ipykernel_23388\\1339983223.py\", line 300, in transcribe_audio\n",
      "    result = model.transcribe(temp_audio_path)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\whisper\\transcribe.py\", line 279, in transcribe\n",
      "    result: DecodingResult = decode_with_fallback(mel_segment)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\whisper\\transcribe.py\", line 195, in decode_with_fallback\n",
      "    decode_result = model.decode(segment, options)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\whisper\\decoding.py\", line 824, in decode\n",
      "    result = DecodingTask(model, options).run(mel)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\whisper\\decoding.py\", line 737, in run\n",
      "    tokens, sum_logprobs, no_speech_probs = self._main_loop(audio_features, tokens)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\whisper\\decoding.py\", line 687, in _main_loop\n",
      "    logits = self.inference.logits(tokens, audio_features)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\whisper\\decoding.py\", line 163, in logits\n",
      "    return self.model.decoder(tokens, audio_features, kv_cache=self.kv_cache)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\whisper\\model.py\", line 211, in forward\n",
      "    x = block(x, xa, mask=self.mask, kv_cache=kv_cache)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\whisper\\model.py\", line 136, in forward\n",
      "    x = x + self.attn(self.attn_ln(x), mask=mask, kv_cache=kv_cache)[0]\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\whisper\\model.py\", line 90, in forward\n",
      "    wv, qk = self.qkv_attention(q, k, v, mask)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\whisper\\model.py\", line 104, in qkv_attention\n",
      "    qk = qk + mask[:n_ctx, :n_ctx]\n",
      "RuntimeError: The size of tensor a (7) must match the size of tensor b (3) at non-singleton dimension 3\n",
      "2024-05-03 22:39:37,676 - INFO - 127.0.0.1 - - [03/May/2024 22:39:37] \"POST /transcribe HTTP/1.1\" 200 -\n",
      "2024-05-03 22:39:40,775 - ERROR - Exception on /transcribe [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\flask\\app.py\", line 1473, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\flask\\app.py\", line 883, in full_dispatch_request\n",
      "    return self.finalize_request(rv)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\flask\\app.py\", line 902, in finalize_request\n",
      "    response = self.make_response(rv)\n",
      "  File \"d:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\lib\\site-packages\\flask\\app.py\", line 1174, in make_response\n",
      "    raise TypeError(\n",
      "TypeError: The view function for 'transcribe_audio' did not return a valid response. The function either returned None or ended without a return statement.\n",
      "2024-05-03 22:39:40,777 - INFO - 127.0.0.1 - - [03/May/2024 22:39:40] \"\u001b[35m\u001b[1mPOST /transcribe HTTP/1.1\u001b[0m\" 500 -\n",
      "2024-05-03 22:39:44,704 - INFO - 127.0.0.1 - - [03/May/2024 22:39:44] \"POST /transcribe HTTP/1.1\" 200 -\n",
      "2024-05-03 22:39:56,917 - INFO - 127.0.0.1 - - [03/May/2024 22:39:56] \"POST /transcribe HTTP/1.1\" 200 -\n",
      "2024-05-03 22:39:58,744 - INFO - 127.0.0.1 - - [03/May/2024 22:39:58] \"POST /transcribe HTTP/1.1\" 200 -\n",
      "2024-05-03 22:40:01,630 - INFO - 127.0.0.1 - - [03/May/2024 22:40:01] \"POST /transcribe HTTP/1.1\" 200 -\n",
      "2024-05-03 22:40:04,290 - INFO - 127.0.0.1 - - [03/May/2024 22:40:04] \"POST /transcribe HTTP/1.1\" 200 -\n",
      "2024-05-03 22:40:06,641 - INFO - 127.0.0.1 - - [03/May/2024 22:40:06] \"POST /transcribe HTTP/1.1\" 200 -\n",
      "2024-05-03 22:40:09,877 - INFO - 127.0.0.1 - - [03/May/2024 22:40:09] \"POST /transcribe HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import time\n",
    "from flask import Flask, request, jsonify\n",
    "import tempfile\n",
    "import whisper\n",
    "import numpy as np\n",
    "import os\n",
    "import librosa \n",
    "import pyaudio\n",
    "# Setup Flask app\n",
    "app = Flask(__name__)\n",
    " # Allow cross-origin requests\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Load the Whisper model once to save resources\n",
    "model = whisper.load_model(\"base\")\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return \"\"\"<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Real-Time Speech to Text</title>\n",
    "    <style>\n",
    "    body {\n",
    "        font-family: 'Segoe UI', Arial, sans-serif; /* Consistent with Microsoft styling */\n",
    "        background-color: #f4f6f8; /* Soothing light background */\n",
    "        margin: 0;\n",
    "        padding: 0;\n",
    "        color: #444; /* Subtle text color for readability */\n",
    "    }\n",
    "\n",
    "    /* Container for all elements */\n",
    "    .container {\n",
    "        max-width: 640px;\n",
    "        margin: 50px auto;\n",
    "        background-color: #fff;\n",
    "        padding: 20px;\n",
    "        border-radius: 10px;\n",
    "        box-shadow: 0 4px 20px rgba(0, 0, 0, 0.2); /* More pronounced shadow for depth */\n",
    "        overflow: hidden; /* Ensures no child element leaks out */\n",
    "    }\n",
    "\n",
    "    /* Header Styles */\n",
    "    .header {\n",
    "        background-image: linear-gradient(135deg, #00aff0 0%, #0061a8 100%);\n",
    "        color: #ffffff;\n",
    "        padding: 20px;\n",
    "        text-align: center;\n",
    "        font-size: 26px;\n",
    "        font-weight: bold;\n",
    "        border-radius: 10px 10px 0 0;\n",
    "    }\n",
    "\n",
    "    /* Call Info Section */\n",
    "    .call-info {\n",
    "        padding: 20px;\n",
    "        display: grid;\n",
    "        grid-template-columns: repeat(2, 1fr);\n",
    "        gap: 10px;\n",
    "        font-size: 16px;\n",
    "        border-bottom: 1px solid #d0e1f9; /* Light blue border */\n",
    "    }\n",
    "\n",
    "    /* Video Call Section */\n",
    "    .video-call {\n",
    "        display: flex;\n",
    "        justify-content: space-around;\n",
    "        margin-top: 20px;\n",
    "        padding: 10px;\n",
    "    }\n",
    "\n",
    "    .video-call div {\n",
    "        flex: 1;\n",
    "        height: 200px;\n",
    "        margin: 10px;\n",
    "        background: rgba(0,0,0,0.1);\n",
    "        border-radius: 10px;\n",
    "        background-size: cover;\n",
    "        background-position: center;\n",
    "        transition: transform 0.3s ease-in-out;\n",
    "    }\n",
    "\n",
    "    .video-call div:hover {\n",
    "        transform: scale(1.05); /* Slight zoom on hover */\n",
    "    }\n",
    "\n",
    "    /* Transcription Box */\n",
    "    .transcription {\n",
    "        background-color: #e8f0fe;\n",
    "        border: 1px solid #c6d8f0;\n",
    "        margin-top: 20px;\n",
    "        padding: 20px;\n",
    "        border-radius: 8px;\n",
    "        font-size: 14px;\n",
    "        box-shadow: inset 0 2px 4px rgba(0,0,0,0.1);\n",
    "    }\n",
    "\n",
    "    /* Button Styles */\n",
    "    .button-container {\n",
    "        margin-top: 20px;\n",
    "        text-align: center;\n",
    "    }\n",
    "\n",
    "    button {\n",
    "        background-color: #00aff0;\n",
    "        color: white;\n",
    "        border: none;\n",
    "        border-radius: 5px;\n",
    "        padding: 10px 20px;\n",
    "        font-size: 16px;\n",
    "        cursor: pointer;\n",
    "        transition: background-color 0.3s, box-shadow 0.3s;\n",
    "    }\n",
    "\n",
    "    button:hover {\n",
    "        background-color: #0077cc;\n",
    "        box-shadow: 0 2px 10px rgba(0, 0, 0, 0.2);\n",
    "    }\n",
    "    /* Add Skype-like video call elements */\n",
    "    .video-call {\n",
    "        display: flex;\n",
    "        flex-wrap: wrap;\n",
    "        justify-content: center; /* Centering the video elements */\n",
    "        margin-top: 20px;\n",
    "    }\n",
    "    .video-call div {\n",
    "        width: 50%; /* Each video takes half the width of the container */\n",
    "        height: 200px; /* Fixed height for video elements */\n",
    "        background-color: #333; /* Dark background for video elements */\n",
    "        border-radius: 10px; /* Rounded corners for video elements */\n",
    "        margin: 10px; /* Margin around video elements for spacing */\n",
    "        background-size: cover;\n",
    "        background-position: center; /* Center the background images */\n",
    "    }\n",
    "    .video-call div:first-child {\n",
    "        background-image: url('/static/Images/man1_livecall.jfif'); /* Assuming the path is correct */\n",
    "    }\n",
    "    .video-call div:last-child {\n",
    "        background-image: url('/static/Images/man2_livecall.jfif'); /* Assuming the path is correct */\n",
    "    }\n",
    "</style>\n",
    "\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"container\">\n",
    "        <div class=\"header\">\n",
    "            <h1>Skype Call</h1>\n",
    "        </div>\n",
    "        <div class=\"call-info\">\n",
    "            <div><strong>Call ID:</strong> 1234567890</div>\n",
    "            <div><strong>Caller:</strong> John Doe</div>\n",
    "            <div><strong>Duration:</strong> <span id=\"callDuration\">00:00:00</span></div>\n",
    "            <div><strong>Call Volume (RMS):</strong> <span id=\"vol\">0.00</span></div>\n",
    "        </div>\n",
    "        <div class=\"video-call\">\n",
    "            <div></div>\n",
    "            <div></div>\n",
    "        </div>\n",
    "        <div class=\"transcription\">\n",
    "            <p id=\"transcription\">Transcription appears here...</p>\n",
    "        </div>\n",
    "        <div class=\"button-container\">\n",
    "            <button id=\"endCall\">End Call</button>\n",
    "        </div>\n",
    "    </div>\n",
    "\n",
    "\n",
    "    <script>\n",
    "    let audioContext, microphone, mediaRecorder, audioChunks = [];\n",
    "    let lastSoundTimestamp = Date.now();\n",
    "    let chunking = 4000;\n",
    "    const silenceThreshold = 500; // Time in milliseconds to define silence duration\n",
    "    let hasSoundBeenDetected = false;  // Indicates if sound has been detected in the current chunk\n",
    "\n",
    "    navigator.mediaDevices.getUserMedia({ audio: true })\n",
    "        .then(stream => {\n",
    "            audioContext = new AudioContext();\n",
    "            microphone = audioContext.createMediaStreamSource(stream);\n",
    "            mediaRecorder = new MediaRecorder(stream);\n",
    "\n",
    "            mediaRecorder.ondataavailable = event => {\n",
    "                if (event.data.size > 0) {\n",
    "                    console.log(\"Audio incoming. Audio incoming. Chunk pushed. No of Chunks = \"+audioChunks.length+1);\n",
    "                    audioChunks.push(event.data);\n",
    "                }\n",
    "            };\n",
    "\n",
    "            mediaRecorder.start(chunking); \n",
    "\n",
    "            const processor = audioContext.createScriptProcessor(2048, 1, 1);\n",
    "            microphone.connect(processor);\n",
    "            processor.connect(audioContext.destination);\n",
    "\n",
    "            processor.onaudioprocess = function(event) {\n",
    "                var input = event.inputBuffer.getChannelData(0);\n",
    "                var sum = 0;\n",
    "                for (var i = 0; i < input.length; ++i) {\n",
    "                    sum += input[i] * input[i];\n",
    "                }\n",
    "                var rms = Math.sqrt(sum / input.length);\n",
    "                \n",
    "                document.getElementById('vol').textContent = rms;\n",
    "                if (rms >= 0.01) {\n",
    "                    lastSoundTimestamp = Date.now();\n",
    "                    hasSoundBeenDetected = true;\n",
    "                } else if ((Date.now() - lastSoundTimestamp) > silenceThreshold && hasSoundBeenDetected) {\n",
    "                    console.log(\"Detected silence after sound, stop the recorder and prepare to send data\");\n",
    "                    mediaRecorder.stop();\n",
    "                }\n",
    "            };\n",
    "\n",
    "            mediaRecorder.onstop = () => {\n",
    "                if (hasSoundBeenDetected) {\n",
    "                    sendAudioToServer();\n",
    "                    hasSoundBeenDetected = false;  // Reset for the next chunk\n",
    "                }\n",
    "                audioChunks = []; // Clear the buffer after sending\n",
    "                mediaRecorder.start(chunking); // Restart recording after processing\n",
    "            };\n",
    "        })\n",
    "        .catch(error => console.error('Error accessing media devices.', error));\n",
    "\n",
    "    function sendAudioToServer() {\n",
    "        const audioBlob = new Blob(audioChunks, { type: 'audio/wav' });\n",
    "        const formData = new FormData();\n",
    "        formData.append('audio', audioBlob, 'file.wav');\n",
    "\n",
    "        fetch('/transcribe', {\n",
    "            method: 'POST',\n",
    "            body: formData\n",
    "        })\n",
    "        .then(response => response.json())\n",
    "        .then(data => {\n",
    "            document.getElementById('transcription').textContent = data.transcription;\n",
    "            console.log(\"Transcription:\", data.transcription);\n",
    "        })\n",
    "        .catch(console.error);\n",
    "    }\n",
    "\n",
    "    document.getElementById('endCall').addEventListener('click', function() {\n",
    "        // Simulate ending the call\n",
    "        alert('Call ended');\n",
    "        window.location.reload(); // Reload the page to reset the call\n",
    "    });\n",
    "\n",
    "    // Example of updating call duration and volume dynamically\n",
    "    setInterval(function() {\n",
    "        let durationElement = document.getElementById('callDuration');\n",
    "        let currentTime = durationElement.textContent;\n",
    "        let newTime = new Date(new Date('1970/01/01 ' + currentTime) .getTime() + 1000);\n",
    "        durationElement.textContent = newTime.toTimeString().substr(0, 8);\n",
    "    }, 1000);\n",
    "\n",
    "    \n",
    "\n",
    "</script>\n",
    "\n",
    "\n",
    "</body>\n",
    "</html>\n",
    "\n",
    "\n",
    "            \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def format_list(lst:list):\n",
    "    string = ''\n",
    "    ctr = 0\n",
    "    for c in lst:\n",
    "        string += (c + \" \")\n",
    "        if(ctr > 6):\n",
    "            string += (\"\\n\")\n",
    "            ctr = 0\n",
    "        ctr += 1\n",
    "    return string\n",
    "\n",
    "conversation_history = []\n",
    "@app.route('/transcribe', methods=['POST'])\n",
    "def transcribe_audio():\n",
    "    global conversation_history\n",
    "    \n",
    "\n",
    "    audio_file = request.files['audio']\n",
    "    \n",
    "    temp_dir = tempfile.mkdtemp(dir=\"D:\\\\DEV\\\\WebdevFolder\\\\RealEstateAI\")\n",
    "    try:\n",
    "        # Save the audio file to a temporary file\n",
    "        temp_audio_path = os.path.join(temp_dir, audio_file.filename)\n",
    "        audio_file.save(temp_audio_path)\n",
    "\n",
    "       \n",
    "        result = model.transcribe(temp_audio_path)\n",
    "        \n",
    "        conversation_history.append( result['text'])\n",
    "        conversation_history_str = format_list(conversation_history)\n",
    "        return jsonify({\"transcription\": conversation_history_str})\n",
    "    except Exception as e:\n",
    "        logging.exception(\"An error occurred during transcription\")\n",
    "        \n",
    "    finally:\n",
    "        time.sleep(10)\n",
    "        # Cleanup: Remove temporary files\n",
    "        os.remove(temp_audio_path)\n",
    "        os.rmdir(temp_dir)\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=False, port=5000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
