{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_community.document_loaders import WebBaseLoader, PyPDFLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from groq import Groq\n",
    "import streamlit as st\n",
    "api_keys = ['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','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',\n",
    "            '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','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']\n",
    "\n",
    "client = Groq(\n",
    "    \n",
    "    api_key = random.choice(api_keys)\n",
    ")\n",
    "\n",
    "llm = ChatGroq(groq_api_key = client.api_key,\n",
    "               model_name = \"llama3-70b-8192\",temperature=0.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.oauth2.credentials import Credentials\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "from googleapiclient.discovery import build\n",
    "import os.path\n",
    "import pickle\n",
    "\n",
    "# Define the scope\n",
    "SCOPES = ['https://www.googleapis.com/auth/drive']\n",
    "credentials_path = \"C:\\\\DEV\\\\WebdevFolder\\\\client_secret_291175256673-gr5p5vf3pi2h0m46h5qnd3ila4iitfqs.apps.googleusercontent.com.json\"\n",
    "def authenticate_google_drive():\n",
    "    creds = None\n",
    "    if os.path.exists('token.json'):\n",
    "        creds = Credentials.from_authorized_user_file('token.json', SCOPES)\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(credentials_path, SCOPES)\n",
    "            creds = flow.run_local_server(port=0)\n",
    "        with open('token.json', 'w') as token:\n",
    "            token.write(creds.to_json())\n",
    "    return build('drive', 'v3', credentials=creds)\n",
    "\n",
    "service = authenticate_google_drive()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_all_files(service):\n",
    "    all_files = {}\n",
    "    page_token = None\n",
    "\n",
    "    while True:\n",
    "        response = service.files().list(\n",
    "            pageSize=100,\n",
    "            fields=\"nextPageToken, files(id, name, mimeType)\",\n",
    "            pageToken=page_token\n",
    "        ).execute()\n",
    "\n",
    "        files = response.get('files', [])\n",
    "        for file in files:\n",
    "            all_files[file['name']] = (file['id'], file['mimeType'])\n",
    "\n",
    "        page_token = response.get('nextPageToken')\n",
    "        if not page_token:\n",
    "            break\n",
    "\n",
    "    return all_files\n",
    "\n",
    "# Retrieve all files and store them in a dictionary\n",
    "all_files_dict = list_all_files(service)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13216"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_files_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Initialize the HuggingFaceEmbeddings with a specific model\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': True, 'batch_size': 224}\n",
    "\n",
    "# Create an instance of HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "# Prepare file descriptions for embedding\n",
    "file_descriptions = [f\"Name: {name}, ID: {details[0]}, Type: {details[1]}\" for name, details in all_files_dict.items()]\n",
    "\n",
    "# Generate embeddings for the file descriptions\n",
    "document_embeddings = embeddings.embed_documents(file_descriptions)\n",
    "\n",
    "# Display the first few embeddings to verify\n",
    "print(document_embeddings[:5])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
