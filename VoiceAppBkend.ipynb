{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install flask whisper\n",
    "#! pip install git+https://github.com/openai/whisper.git -q\n",
    "# !pip install ffmpeg\n",
    "# !pip install ffmpeg-python\n",
    "#!pip install openai\n",
    "#!pip install transformers\n",
    "#!pip install librosa pyaudio\n",
    "#!pip install --force-reinstall scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-05 17:49:54,948 - INFO - \u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on http://127.0.0.1:5000\n",
      "2024-05-05 17:49:54,949 - INFO - \u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "2024-05-05 17:49:58,221 - INFO - 127.0.0.1 - - [05/May/2024 17:49:58] \"GET / HTTP/1.1\" 200 -\n",
      "2024-05-05 17:50:05,267 - INFO - 127.0.0.1 - - [05/May/2024 17:50:05] \"GET /static/Images/man1_livecall.jfif HTTP/1.1\" 200 -\n",
      "2024-05-05 17:50:05,280 - INFO - 127.0.0.1 - - [05/May/2024 17:50:05] \"GET /static/Images/man2_livecall.jfif HTTP/1.1\" 200 -\n",
      "2024-05-05 17:50:33,304 - INFO - 0\n",
      "2024-05-05 17:51:59,731 - INFO - 127.0.0.1 - - [05/May/2024 17:51:59] \"POST /transcribe HTTP/1.1\" 200 -\n",
      "2024-05-05 17:52:23,987 - INFO - 0\n",
      "2024-05-05 17:52:24,184 - ERROR - An error occurred during transcription\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\pc-user1\\AppData\\Local\\Temp\\ipykernel_13740\\3281663175.py\", line 343, in transcribe_audio\n",
      "    sound = AudioSegment.from_file(temp_audio_path)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\DEV\\WebdevFolder\\RealEstateAI\\.venv\\Lib\\site-packages\\pydub\\audio_segment.py\", line 773, in from_file\n",
      "    raise CouldntDecodeError(\n",
      "pydub.exceptions.CouldntDecodeError: Decoding failed. ffmpeg returned error code: 3199971767\n",
      "\n",
      "Output from ffmpeg/avlib:\n",
      "\n",
      "ffmpeg version 2024-05-02-git-71669f2ad5-full_build-www.gyan.dev Copyright (c) 2000-2024 the FFmpeg developers\n",
      "  built with gcc 13.2.0 (Rev5, Built by MSYS2 project)\n",
      "  configuration: --enable-gpl --enable-version3 --enable-static --disable-w32threads --disable-autodetect --enable-fontconfig --enable-iconv --enable-gnutls --enable-libxml2 --enable-gmp --enable-bzlib --enable-lzma --enable-libsnappy --enable-zlib --enable-librist --enable-libsrt --enable-libssh --enable-libzmq --enable-avisynth --enable-libbluray --enable-libcaca --enable-sdl2 --enable-libaribb24 --enable-libaribcaption --enable-libdav1d --enable-libdavs2 --enable-libuavs3d --enable-libxevd --enable-libzvbi --enable-librav1e --enable-libsvtav1 --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxavs2 --enable-libxeve --enable-libxvid --enable-libaom --enable-libjxl --enable-libopenjpeg --enable-libvpx --enable-mediafoundation --enable-libass --enable-frei0r --enable-libfreetype --enable-libfribidi --enable-libharfbuzz --enable-liblensfun --enable-libvidstab --enable-libvmaf --enable-libzimg --enable-amf --enable-cuda-llvm --enable-cuvid --enable-dxva2 --enable-d3d11va --enable-d3d12va --enable-ffnvcodec --enable-libvpl --enable-nvdec --enable-nvenc --enable-vaapi --enable-libshaderc --enable-vulkan --enable-libplacebo --enable-opencl --enable-libcdio --enable-libgme --enable-libmodplug --enable-libopenmpt --enable-libopencore-amrwb --enable-libmp3lame --enable-libshine --enable-libtheora --enable-libtwolame --enable-libvo-amrwbenc --enable-libcodec2 --enable-libilbc --enable-libgsm --enable-libopencore-amrnb --enable-libopus --enable-libspeex --enable-libvorbis --enable-ladspa --enable-libbs2b --enable-libflite --enable-libmysofa --enable-librubberband --enable-libsoxr --enable-chromaprint\n",
      "  libavutil      59. 16.101 / 59. 16.101\n",
      "  libavcodec     61.  5.103 / 61.  5.103\n",
      "  libavformat    61.  3.103 / 61.  3.103\n",
      "  libavdevice    61.  2.100 / 61.  2.100\n",
      "  libavfilter    10.  2.101 / 10.  2.101\n",
      "  libswscale      8.  2.100 /  8.  2.100\n",
      "  libswresample   5.  2.100 /  5.  2.100\n",
      "  libpostproc    58.  2.100 / 58.  2.100\n",
      "[in#0 @ 0000022b8fb6b480] Error opening input: Invalid data found when processing input\n",
      "Error opening input file C:\\DEV\\WebdevFolder\\RealEstateAI\\tmp9_qr_0ka\\file.wav.\n",
      "Error opening input files: Invalid data found when processing input\n",
      "\n",
      "2024-05-05 17:52:24,188 - INFO - 127.0.0.1 - - [05/May/2024 17:52:24] \"POST /transcribe HTTP/1.1\" 200 -\n",
      "2024-05-05 17:53:25,790 - INFO - 127.0.0.1 - - [05/May/2024 17:53:25] \"GET / HTTP/1.1\" 200 -\n",
      "2024-05-05 17:53:25,795 - INFO - 127.0.0.1 - - [05/May/2024 17:53:25] \"POST /reset HTTP/1.1\" 200 -\n",
      "2024-05-05 17:53:28,284 - INFO - 127.0.0.1 - - [05/May/2024 17:53:28] \"GET /static/Images/man1_livecall.jfif HTTP/1.1\" 200 -\n",
      "2024-05-05 17:53:28,292 - INFO - 127.0.0.1 - - [05/May/2024 17:53:28] \"GET /static/Images/man2_livecall.jfif HTTP/1.1\" 200 -\n",
      "2024-05-05 17:53:38,311 - INFO - 0\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import time\n",
    "from flask import Flask, request, jsonify\n",
    "import tempfile\n",
    "import whisper\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "# Setup Flask app\n",
    "app = Flask(__name__)\n",
    " # Allow cross-origin requests\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Load the Whisper model once to save resources\n",
    "model = whisper.load_model(\"base\")\n",
    "app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024  # for example, to allow up to 16 MB files\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return \"\"\"<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Real-Time Speech to Text</title>\n",
    "    <style>\n",
    "    body {\n",
    "        font-family: 'Segoe UI', Arial, sans-serif; /* Consistent with Microsoft styling */\n",
    "        background-color: #f4f6f8; /* Soothing light background */\n",
    "        margin: 0;\n",
    "        padding: 0;\n",
    "        color: #444; /* Subtle text color for readability */\n",
    "    }\n",
    "\n",
    "    /* Container for all elements */\n",
    "    .container {\n",
    "        max-width: 640px;\n",
    "        margin: 50px auto;\n",
    "        background-color: #fff;\n",
    "        padding: 20px;\n",
    "        border-radius: 10px;\n",
    "        box-shadow: 0 4px 20px rgba(0, 0, 0, 0.2); /* More pronounced shadow for depth */\n",
    "        overflow: hidden; /* Ensures no child element leaks out */\n",
    "    }\n",
    "\n",
    "    /* Header Styles */\n",
    "    .header {\n",
    "        background-image: linear-gradient(135deg, #00aff0 0%, #0061a8 100%);\n",
    "        color: #ffffff;\n",
    "        padding: 20px;\n",
    "        text-align: center;\n",
    "        font-size: 26px;\n",
    "        font-weight: bold;\n",
    "        border-radius: 10px 10px 0 0;\n",
    "    }\n",
    "\n",
    "    /* Call Info Section */\n",
    "    .call-info {\n",
    "        padding: 20px;\n",
    "        display: grid;\n",
    "        grid-template-columns: repeat(2, 1fr);\n",
    "        gap: 10px;\n",
    "        font-size: 16px;\n",
    "        border-bottom: 1px solid #d0e1f9; /* Light blue border */\n",
    "    }\n",
    "\n",
    "    /* Video Call Section */\n",
    "    .video-call {\n",
    "        display: flex;\n",
    "        justify-content: space-around;\n",
    "        margin-top: 20px;\n",
    "        padding: 10px;\n",
    "    }\n",
    "\n",
    "    .video-call div {\n",
    "        flex: 1;\n",
    "        height: 200px;\n",
    "        margin: 10px;\n",
    "        background: rgba(0,0,0,0.1);\n",
    "        border-radius: 10px;\n",
    "        background-size: cover;\n",
    "        background-position: center;\n",
    "        transition: transform 0.3s ease-in-out;\n",
    "    }\n",
    "\n",
    "    .video-call div:hover {\n",
    "        transform: scale(1.05); /* Slight zoom on hover */\n",
    "    }\n",
    "\n",
    "    /* Transcription Box */\n",
    "    .transcription {\n",
    "        background-color: #e8f0fe;\n",
    "        border: 1px solid #c6d8f0;\n",
    "        margin-top: 20px;\n",
    "        padding: 20px;\n",
    "        border-radius: 8px;\n",
    "        font-size: 14px;\n",
    "        box-shadow: inset 0 2px 4px rgba(0,0,0,0.1);\n",
    "    }\n",
    "\n",
    "    /* Button Styles */\n",
    "    .button-container {\n",
    "        margin-top: 20px;\n",
    "        text-align: center;\n",
    "    }\n",
    "\n",
    "    button {\n",
    "        background-color: #00aff0;\n",
    "        color: white;\n",
    "        border: none;\n",
    "        border-radius: 5px;\n",
    "        padding: 10px 20px;\n",
    "        font-size: 16px;\n",
    "        cursor: pointer;\n",
    "        transition: background-color 0.3s, box-shadow 0.3s;\n",
    "    }\n",
    "\n",
    "    button:hover {\n",
    "        background-color: #0077cc;\n",
    "        box-shadow: 0 2px 10px rgba(0, 0, 0, 0.2);\n",
    "    }\n",
    "    /* Add Skype-like video call elements */\n",
    "    .video-call {\n",
    "        display: flex;\n",
    "        flex-wrap: wrap;\n",
    "        justify-content: center; /* Centering the video elements */\n",
    "        margin-top: 20px;\n",
    "    }\n",
    "    .video-call div {\n",
    "        width: 50%; /* Each video takes half the width of the container */\n",
    "        height: 200px; /* Fixed height for video elements */\n",
    "        background-color: #333; /* Dark background for video elements */\n",
    "        border-radius: 10px; /* Rounded corners for video elements */\n",
    "        margin: 10px; /* Margin around video elements for spacing */\n",
    "        background-size: cover;\n",
    "        background-position: center; /* Center the background images */\n",
    "    }\n",
    "    .video-call div:first-child {\n",
    "        background-image: url('/static/Images/man1_livecall.jfif'); /* Assuming the path is correct */\n",
    "    }\n",
    "    .video-call div:last-child {\n",
    "        background-image: url('/static/Images/man2_livecall.jfif'); /* Assuming the path is correct */\n",
    "    }\n",
    "</style>\n",
    "\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"container\">\n",
    "        <div class=\"header\">\n",
    "            <h1>Live Call</h1>\n",
    "        </div>\n",
    "        <div class=\"call-info\">\n",
    "            <div><strong>Call ID:</strong> 1234567890</div>\n",
    "            <div><strong>Caller:</strong> John Doe</div>\n",
    "            <div><strong>Duration:</strong> <span id=\"callDuration\">00:00:00</span></div>\n",
    "            <div><strong>Call Volume (RMS):</strong> <span id=\"vol\">0.00</span></div>\n",
    "        </div>\n",
    "        <div class=\"video-call\">\n",
    "            <div></div>\n",
    "            <div></div>\n",
    "        </div>\n",
    "        <div class=\"transcription\">\n",
    "            <p id=\"transcription\">Transcription appears here...</p>\n",
    "        </div>\n",
    "        <div class=\"button-container\">\n",
    "            <button id=\"endCall\">End Call</button>\n",
    "            <button id=\"muteButton\" onclick=\"toggleMute()\">Mute</button>\n",
    "        </div>\n",
    "    </div>\n",
    "\n",
    "\n",
    "<script>\n",
    "    let audioContext, microphone, mediaRecorder, audioChunks = [];\n",
    "    let lastSoundTimestamp = Date.now();\n",
    "    let chunking = 2000;\n",
    "    let isMuted = false; \n",
    "    let volThreshold = 0.0011;\n",
    "    let mediaStream;\n",
    "    const silenceThreshold = 1000; // Time in milliseconds to define silence duration\n",
    "    let hasSoundBeenDetected = false;  // Indicates if sound has been detected in the current chunk\n",
    "    let volspan = document.getElementById('vol');\n",
    "    navigator.mediaDevices.getUserMedia({ audio: true })\n",
    "        .then(stream => {\n",
    "            mediaStream = stream;\n",
    "            audioContext = new AudioContext();\n",
    "            microphone = audioContext.createMediaStreamSource(stream);\n",
    "            //let options = {mimeType: 'audio/wav'};\n",
    "            mediaRecorder = new MediaRecorder(stream);\n",
    "\n",
    "            mediaRecorder.ondataavailable = event => {\n",
    "                const volume = parseFloat(volspan.textContent);\n",
    "                if (event.data.size > 0 && !isNaN(volume) && volume >= volThreshold) {\n",
    "                    console.log(\"Audio incoming. Audio incoming. Chunk pushed. No of Chunks = \" + (audioChunks.length + 1));\n",
    "                    audioChunks.push(event.data);\n",
    "                }\n",
    "            };\n",
    "\n",
    "            mediaRecorder.start(chunking); \n",
    "\n",
    "            const processor = audioContext.createScriptProcessor(2048, 1, 1);\n",
    "            microphone.connect(processor);\n",
    "            processor.connect(audioContext.destination);\n",
    "\n",
    "            processor.onaudioprocess = function(event) {\n",
    "                var input = event.inputBuffer.getChannelData(0);\n",
    "                var sum = 0;\n",
    "                for (var i = 0; i < input.length; ++i) {\n",
    "                    sum += input[i] * input[i];\n",
    "                }\n",
    "                var rms = Math.sqrt(sum / input.length);\n",
    "                \n",
    "                volspan.textContent = rms.toFixed(5);\n",
    "                if (rms >= volThreshold) {\n",
    "                    lastSoundTimestamp = Date.now();\n",
    "                    hasSoundBeenDetected = true;\n",
    "                } else if ((Date.now() - lastSoundTimestamp) > silenceThreshold && hasSoundBeenDetected) {\n",
    "                    console.log(\"Detected silence after sound, stop the recorder and prepare to send data\");\n",
    "                    mediaRecorder.stop();\n",
    "                }\n",
    "            };\n",
    "\n",
    "            mediaRecorder.onstop = () => {\n",
    "                if (hasSoundBeenDetected) {\n",
    "                    sendAudioToServer();\n",
    "                    hasSoundBeenDetected = false;  // Reset for the next chunk\n",
    "                }\n",
    "                audioChunks = []; // Clear the buffer after sending\n",
    "                mediaRecorder.start(chunking); // Restart recording after processing\n",
    "            };\n",
    "        })\n",
    "        .catch(error => console.error('Error accessing media devices.', error));\n",
    "\n",
    "    function sendAudioToServer() {\n",
    "        const audioBlob = new Blob(audioChunks, { type: \"audio/wav\" });\n",
    "        console.log(\"Sending audio blob with size:\", audioBlob.size); // Check the size\n",
    "        if(audioBlob.size === 0){\n",
    "            console.error(\"Audio blob is empty.\");\n",
    "            \n",
    "        }else {\n",
    "            const formData = new FormData();\n",
    "            formData.append('audio', audioBlob, \"file.wav\");\n",
    "\n",
    "            fetch('/transcribe', {\n",
    "                method: 'POST',\n",
    "                body: formData\n",
    "            })\n",
    "            .then(response => response.json())\n",
    "            .then(data => {\n",
    "                document.getElementById('transcription').textContent = data.transcription;\n",
    "                console.log(\"Transcription:\", data.transcription);\n",
    "            })\n",
    "            .catch(console.error);\n",
    "        }\n",
    "    }\n",
    "\n",
    "    document.getElementById('endCall').addEventListener('click', function() {\n",
    "        // Simulate ending the call\n",
    "        alert('Call ended');\n",
    "        window.location.reload(); // Reload the page to reset the call\n",
    "    });\n",
    "\n",
    "    // Example of updating call duration and volume dynamically\n",
    "    setInterval(function() {\n",
    "        let durationElement = document.getElementById('callDuration');\n",
    "        let currentTime = durationElement.textContent;\n",
    "        let newTime = new Date(new Date('1970/01/01 ' + currentTime) .getTime() + 1000);\n",
    "        durationElement.textContent = newTime.toTimeString().substr(0, 8);\n",
    "    }, 1000);\n",
    "    function toggleMute() {\n",
    "        if (isMuted) {\n",
    "            unmuteAudio();\n",
    "        } else {\n",
    "            muteAudio();\n",
    "        }\n",
    "        isMuted = !isMuted;  // Toggle the muted state\n",
    "        document.getElementById('muteButton').textContent = isMuted ? \"Unmute\" : \"Mute\";\n",
    "    }\n",
    "\n",
    "    function muteAudio() {\n",
    "        mediaStream.getAudioTracks().forEach(track => track.enabled = false);\n",
    "    }\n",
    "\n",
    "    function unmuteAudio() {\n",
    "        mediaStream.getAudioTracks().forEach(track => track.enabled = true);\n",
    "    }\n",
    "    document.getElementById('endCall').addEventListener('click', function() {\n",
    "        fetch('/reset', {\n",
    "            method: 'POST'\n",
    "        })\n",
    "        .then(response => response.json())\n",
    "        .then(data => {\n",
    "            console.log(data.message); // Log the response message\n",
    "            window.location.reload(); // Reload the page to reset the call\n",
    "        })\n",
    "        .catch(console.error);\n",
    "    });\n",
    "\n",
    "\n",
    "</script>\n",
    "\n",
    "\n",
    "</body>\n",
    "</html>\n",
    "\n",
    "\n",
    "            \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from pydub import AudioSegment \n",
    "def format_list(lst:list):\n",
    "    string = ''\n",
    "    ctr = 0\n",
    "    for c in lst:\n",
    "        string += (c + \" \")\n",
    "        if(ctr > 6):\n",
    "            string += (\"\\n\")\n",
    "            ctr = 0\n",
    "        ctr += 1\n",
    "    return string\n",
    "\n",
    "conversation_history = []\n",
    "conversation_history_str = \"\"\n",
    "@app.route('/transcribe', methods=['POST'])\n",
    "def transcribe_audio():\n",
    "    global conversation_history, conversation_history_str\n",
    "    \n",
    "\n",
    "    audio_file = request.files['audio']\n",
    "    logging.info(audio_file.content_length)\n",
    "    temp_dir = tempfile.mkdtemp(dir=\"C:\\\\DEV\\\\WebdevFolder\\\\RealEstateAI\")\n",
    "    processed_path=None\n",
    "    try:\n",
    "        # Save the audio file to a temporary file\n",
    "        temp_audio_path = os.path.join(temp_dir, audio_file.filename)\n",
    "        audio_file.save(temp_audio_path)\n",
    "        \n",
    "        # Use pydub to ensure the audio is mono and at the correct sample rate\n",
    "        sound = AudioSegment.from_file(temp_audio_path)\n",
    "        sound = sound.set_frame_rate(16000).set_channels(1)\n",
    "        processed_path = os.path.join(temp_dir, \"processed_\" + audio_file.filename)\n",
    "        sound.export(processed_path, format=\"wav\")\n",
    "        \n",
    "        # Transcribe the audio\n",
    "        result = model.transcribe(processed_path, fp16=False)\n",
    "        \n",
    "        conversation_history.append( result['text'])\n",
    "        conversation_history_str = format_list(conversation_history)\n",
    "        return jsonify({\"transcription\": conversation_history_str})\n",
    "    except Exception as e:\n",
    "        logging.exception(\"An error occurred during transcription\")\n",
    "        return jsonify({\"transcription\": conversation_history_str})\n",
    "        \n",
    "    finally:\n",
    "        # time.sleep(10)\n",
    "        # Cleanup: Remove temporary files\n",
    "        if(processed_path is not None):\n",
    "            os.remove(processed_path)\n",
    "        if os.path.exists(temp_audio_path):\n",
    "            os.remove(temp_audio_path)\n",
    "        os.rmdir(temp_dir)\n",
    "    \n",
    "@app.route('/reset', methods=['POST'])\n",
    "def reset_conversation_history():\n",
    "    global conversation_history, conversation_history_str\n",
    "    conversation_history = []\n",
    "    conversation_history_str = \"\"\n",
    "    return jsonify({\"message\": \"Conversation history reset successfully\"})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=False, port=5000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
